{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ITO1erJr04W8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/ml/code/transformer_dst')\n",
    "from transformer_dst.TransformerDSTmodel import TransformerDST\n",
    "from pytorch_transformers import AdamW, WarmupLinearSchedule, BertConfig\n",
    "from transformers import BertTokenizer\n",
    "from transformer_dst.utils.data_utils import prepare_dataset, MultiWozDataset, load_dataset\n",
    "from transformer_dst.utils.data_utils import make_slot_meta, domain2id, OP_SET, make_turn_label, postprocessing\n",
    "from transformer_dst.utils.eval_utils import compute_prf, compute_acc, per_domain_join_accuracy\n",
    "from transformer_dst.utils.ckpt_utils import download_ckpt, convert_ckpt_compatible\n",
    "from transformer_dst.TransformerDSTevaluation import model_evaluation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1Tw9bZT04XC"
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hb5jD58hEa5B"
   },
   "outputs": [],
   "source": [
    "def masked_cross_entropy_for_value(logits, target, pad_idx=0):\n",
    "    mask = target.ne(pad_idx)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    log_probs_flat = torch.log(logits_flat)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    losses = losses * mask.float()\n",
    "    loss = losses.sum() / (mask.sum().float())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def save(args, epoch, model, enc_optimizer, dec_optimizer=None):\n",
    "    model_to_save = model.module if hasattr(\n",
    "        model, 'module') else model  # Only save the model it-self\n",
    "    model_file = os.path.join(\n",
    "        args.save_dir, \"model.e{:}.bin\".format(epoch))\n",
    "    torch.save(model_to_save.state_dict(), model_file)\n",
    "\n",
    "    # enc_optim_file = os.path.join(\n",
    "    #     args.save_dir, \"enc_optim.e{:}.bin\".format(epoch))\n",
    "    # torch.save(enc_optimizer.state_dict(), enc_optim_file)\n",
    "    #\n",
    "    # if dec_optimizer is not None:\n",
    "    #     dec_optim_file = os.path.join(\n",
    "    #         args.save_dir, \"dec_optim.e{:}.bin\".format(epoch))\n",
    "    #     torch.save(dec_optimizer.state_dict(), dec_optim_file)\n",
    "\n",
    "\n",
    "def load(args, epoch):\n",
    "    model_file = os.path.join(\n",
    "        args.save_dir, \"model.e{:}.bin\".format(epoch))\n",
    "    model_recover = torch.load(model_file, map_location='cpu')\n",
    "\n",
    "    enc_optim_file = os.path.join(\n",
    "        args.save_dir, \"enc_optim.e{:}.bin\".format(epoch))\n",
    "    enc_recover = torch.load(enc_optim_file, map_location='cpu')\n",
    "    if hasattr(enc_recover, 'state_dict'):\n",
    "        enc_recover = enc_recover.state_dict()\n",
    "\n",
    "    dec_optim_file = os.path.join(\n",
    "        args.save_dir, \"dec_optim.e{:}.bin\".format(epoch))\n",
    "    dec_recover = torch.load(dec_optim_file, map_location='cpu')\n",
    "    if hasattr(dec_recover, 'state_dict'):\n",
    "        dec_recover = dec_recover.state_dict()\n",
    "\n",
    "    return model_recover, enc_recover, dec_recover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yHQi7qZ_xas2"
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "args ={\n",
    "    \"data_root\":'/opt/ml/input/data/train_dataset',\n",
    "    \"data_root_test\":'/opt/ml/input/data/eval_dataset',\n",
    "    \"train_data\":\"train_dials.json\",\n",
    "    \"dev_data\":\"dev_dials.json\",\n",
    "    \"test_data\":\"eval_dials.json\",\n",
    "    \"save_dir\":'outputs',\n",
    "    \"ontology_data\":'/opt/ml/input/data/train_dataset/ontology.json',\n",
    "    \"vocab_path\":'assets/vocab.txt',\n",
    "    \"bert_config_path\":'/opt/ml/code/transformer_dst/utils/bert_ko_small_minimal.json',\n",
    "    \"bert_config\":'dsksd/bert-ko-small-minimal',\n",
    "    \"bert_ckpt_path\":'/opt/ml/code/transformer_dst/assets/dsksd/bert-ko-small-minimal-pytorch_model.bin',\n",
    "    \"random_seed\":42,\n",
    "    \"batch_size\":16,\n",
    "    \"n_epochs\":30,\n",
    "    \"eval_epoch\":1,\n",
    "    \"op_code\":\"4\",\n",
    "    \"slot_token\":\"[SLOT]\",\n",
    "    \"hidden_dropout_prob\":0.1,\n",
    "    \"decoder_teacher_forcing\":1,\n",
    "    \"word_dropout\":0.1,\n",
    "    \"shuffle_p\":0.5,\n",
    "    \"n_history\":1,\n",
    "    \"max_seq_length\":512,\n",
    "    \"msg\":None,\n",
    "    \"exclude_domain\":False,\n",
    "    \"beam_size\":1,\n",
    "    \"min_len\":1,\n",
    "    \"length_penalty\":0,\n",
    "    \"ngram_size\":2,\n",
    "    \"shuffle_state\":False,\n",
    "    # By default, \"decoder\" only attend on a specific [SLOT] position.\n",
    "    # If using this option, the \"decoder\" can access to this group of \"[SLOT] domain slot - value\".\n",
    "    \"use_full_slot\":False,\n",
    "    # Using only D_t in generation\n",
    "    \"use_dt_only\":True,\n",
    "    # w/o re-using dialogue\n",
    "    \"no_dial\":False,\n",
    "    # Using only [CLS]\n",
    "    \"use_cls_only\":False,\n",
    "    \"dropout\":0.1,\n",
    "    \"hidden_dropout_prob\":0.1,\n",
    "    \"attention_probs_dropout_prob\":0.1,\n",
    "    \"forbid_duplicate_ngrams\":True,\n",
    "    \"forbid_ignore_word\":None,\n",
    "    \"use_one_optim\":True,   # 논문에서는 optim 1개 사용\n",
    "    \"enc_lr\":3e-5,\n",
    "    \"enc_warmup\":0.1,\n",
    "    \"num_workers\":0,\n",
    "    \"only_pred_op\":False\n",
    "}\n",
    "\n",
    "args = Namespace(**args)\n",
    "\n",
    "args.train_data_path = os.path.join(args.data_root, args.train_data)\n",
    "args.dev_data_path = os.path.join(args.data_root, args.dev_data)\n",
    "args.test_data_path = os.path.join(args.data_root_test, args.test_data)\n",
    "args.ontology_data = os.path.join(args.data_root, args.ontology_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218,
     "referenced_widgets": [
      "002f715aa4bd444ca40d15d40975b9b6",
      "4df3657b19324b46926ee324d77b890c",
      "1160ad01ed8b4abca76708edfd6f9fff",
      "6405f7f3573049aebb815bccc057e089",
      "db57f006af5446ba8c402d2ecc8b3798",
      "2e4ed712135542a1b75b276fe9dae691",
      "d57b60747cd943568a22cb38eda92fae",
      "50ea6b36f88d4c89bc58e9da3e03b274",
      "be8cff8394b14d928d44c973286e9e19",
      "9a5445823b104e979787a61c19e4ee84",
      "6816d95cb3b840f1bcd8bd169919bf60",
      "836b292e938e42c0a0c2caffd46e786b",
      "b2f70e8ede5f4e63b16c83646d24e196",
      "8fe163c517734c23b6fb15f72952c843",
      "0d063cc9981748359845a690ce4132ce",
      "6e6e2940e6534671947b95268ccf0c38",
      "34ea407dcc794348aa33fc46a19a963a",
      "339aa73d924c4f34b20cf927e3acbad1",
      "7e8c27125cee4105a82fe0ba09e5d77e",
      "e4e299a6984540e686940e7f763606c2",
      "82812bfd15054564876b1cfdcd666c93",
      "a2d36a80b8f144cc815c7c00c6eab9f5",
      "dbb46730360748c98b336e5f24892276",
      "1d91402b3e81484a8e806133ae0b2aa6"
     ]
    },
    "id": "ajXb2IZBEnnb",
    "outputId": "da1403ec-bdd2-4f4c-e813-280e6653c288",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Random Seed: 42\n",
      "{'delete': 0, 'update': 1, 'dontcare': 2, 'carryover': 3}\n",
      "### decoder_teacher_forcing: 1\n",
      "# train examples 46170\n",
      "# dev examples 5075\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(args.save_dir):\n",
    "    os.mkdir(args.save_dir)\n",
    "    print(\"### mkdir {:}\".format(args.save_dir))\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    np.random.seed(args.random_seed + worker_id)\n",
    "\n",
    "n_gpu = 0\n",
    "n_gpu = torch.cuda.device_count()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "if args.random_seed < 0:\n",
    "    print(\"### Pick a random seed\")\n",
    "    args.random_seed = random.sample(list(range(0, 100000)), 1)[0]\n",
    "\n",
    "print(\"### Random Seed: {:}\".format(args.random_seed))\n",
    "np.random.seed(args.random_seed)\n",
    "random.seed(args.random_seed)\n",
    "rng = random.Random(args.random_seed)\n",
    "torch.manual_seed(args.random_seed)\n",
    "\n",
    "if n_gpu > 0:\n",
    "    if args.random_seed >= 0:\n",
    "        torch.cuda.manual_seed(args.random_seed)\n",
    "        torch.cuda.manual_seed_all(args.random_seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.mkdir(args.save_dir)\n",
    "\n",
    "ontology = json.load(open(args.ontology_data))\n",
    "slot_meta, ontology = make_slot_meta(ontology)\n",
    "op2id = OP_SET[args.op_code]\n",
    "print(op2id)\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(args.bert_config)\n",
    "\n",
    "special_tokens = ['[SLOT]', '[NULL]']\n",
    "special_tokens_dict = {'additional_special_tokens': special_tokens}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "train_data, dev_data = load_dataset(args.train_data_path)\n",
    "\n",
    "train_path = os.path.join(args.data_root, \"train.pt\")\n",
    "dev_path = os.path.join(args.data_root, \"dev.pt\")\n",
    "# test_path = os.path.join(args.data_root, \"test.pt\")\n",
    "\n",
    "# if not os.path.exists(test_path):\n",
    "#     test_data_raw = prepare_dataset(data_path=args.test_data_path,\n",
    "#                                     tokenizer=tokenizer,\n",
    "#                                     slot_meta=slot_meta,\n",
    "#                                     n_history=args.n_history,\n",
    "#                                     max_seq_length=args.max_seq_length,\n",
    "#                                     op_code=args.op_code)\n",
    "#     torch.save(test_data_raw, test_path)\n",
    "# else:\n",
    "#     test_data_raw = torch.load(test_path)\n",
    "\n",
    "# print(\"# test examples %d\" % len(test_data_raw))\n",
    "\n",
    "if not os.path.exists(train_path):\n",
    "    train_data_raw = prepare_dataset(data_path=args.train_data_path,\n",
    "                                     data_list=train_data,\n",
    "                                     tokenizer=tokenizer,\n",
    "                                     slot_meta=slot_meta,\n",
    "                                     n_history=args.n_history,\n",
    "                                     max_seq_length=args.max_seq_length,\n",
    "                                     op_code=args.op_code)\n",
    "    torch.save(train_data_raw, train_path)\n",
    "else:\n",
    "    train_data_raw = torch.load(train_path)\n",
    "\n",
    "train_data = MultiWozDataset(train_data_raw,\n",
    "                             tokenizer,\n",
    "                             slot_meta,\n",
    "                             args.max_seq_length,\n",
    "                             rng,\n",
    "                             ontology,\n",
    "                             args.word_dropout,\n",
    "                             args.shuffle_state,\n",
    "                             args.shuffle_p, pad_id=tokenizer.convert_tokens_to_ids(['[PAD]'])[0],\n",
    "                             slot_id=tokenizer.convert_tokens_to_ids(['[SLOT]'])[0],\n",
    "                             decoder_teacher_forcing=args.decoder_teacher_forcing,\n",
    "                             use_full_slot=args.use_full_slot,\n",
    "                             use_dt_only=args.use_dt_only, no_dial=args.no_dial,\n",
    "                             use_cls_only=args.use_cls_only)\n",
    "\n",
    "print(\"# train examples %d\" % len(train_data_raw))\n",
    "\n",
    "if not os.path.exists(dev_path):\n",
    "    dev_data_raw = prepare_dataset(data_path=None,\n",
    "                                   data_list=dev_data,\n",
    "                                   tokenizer=tokenizer,\n",
    "                                   slot_meta=slot_meta,\n",
    "                                   n_history=args.n_history,\n",
    "                                   max_seq_length=args.max_seq_length,\n",
    "                                   op_code=args.op_code)\n",
    "    torch.save(dev_data_raw,  dev_path)\n",
    "else:\n",
    "    dev_data_raw = torch.load(dev_path)\n",
    "\n",
    "print(\"# dev examples %d\" % len(dev_data_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### decoder_teacher_forcing: 1\n"
     ]
    }
   ],
   "source": [
    "train_data = MultiWozDataset(train_data_raw,\n",
    "                                 tokenizer,\n",
    "                                 slot_meta,\n",
    "                                 args.max_seq_length,\n",
    "                                 rng,\n",
    "                                 ontology,\n",
    "                                 args.word_dropout,\n",
    "                                 args.shuffle_state,\n",
    "                                 args.shuffle_p, pad_id=tokenizer.convert_tokens_to_ids(['[PAD]'])[0],\n",
    "                                 slot_id=tokenizer.convert_tokens_to_ids(['[SLOT]'])[0],\n",
    "                                 decoder_teacher_forcing=args.decoder_teacher_forcing,\n",
    "                                 use_full_slot=args.use_full_slot,\n",
    "                                 use_dt_only=args.use_dt_only, no_dial=args.no_dial,\n",
    "                                 use_cls_only=args.use_cls_only)\n",
    "\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data,\n",
    "                              sampler=train_sampler,\n",
    "                              batch_size=args.batch_size,\n",
    "                              collate_fn=train_data.collate_fn,\n",
    "                              num_workers=args.num_workers,\n",
    "                              worker_init_fn=worker_init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [], [[22, 3, 0, 0], [21, 3, 0, 0], [9622, 3, 0, 0], [8139, 7396, 6265, 3]], [[10238, 3, 0, 0], [93, 6756, 3, 0], [7396, 3, 0, 0], [93, 6756, 3, 0]], [[93, 6756, 3, 0]], [], [], [], [], [[20227, 3, 0, 0], [6265, 10806, 3, 0]], [[6265, 11132, 3, 0]], [], []]\n",
      "[22, 3, 0, 0]\n",
      "2 [SEP] [PAD] [PAD]\n",
      "[21, 3, 0, 0]\n",
      "1 [SEP] [PAD] [PAD]\n",
      "[9622, 3, 0, 0]\n",
      "일요일 [SEP] [PAD] [PAD]\n",
      "[8139, 7396, 6265, 3]\n",
      "메이 호텔 서울 [SEP]\n",
      "[10238, 3, 0, 0]\n",
      "비싼 [SEP] [PAD] [PAD]\n",
      "[93, 6756, 3, 0]\n",
      "yes [SEP] [PAD]\n",
      "[7396, 3, 0, 0]\n",
      "호텔 [SEP] [PAD] [PAD]\n",
      "[93, 6756, 3, 0]\n",
      "yes [SEP] [PAD]\n",
      "[93, 6756, 3, 0]\n",
      "yes [SEP] [PAD]\n",
      "[20227, 3, 0, 0]\n",
      "랜드마크 [SEP] [PAD] [PAD]\n",
      "[6265, 10806, 3, 0]\n",
      "서울 서쪽 [SEP] [PAD]\n",
      "[6265, 11132, 3, 0]\n",
      "서울 동쪽 [SEP] [PAD]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.n_epochs):\n",
    "    batch_loss = []\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = [b if (not isinstance(b, int)) and (\n",
    "                    not isinstance(b, dict) and (not isinstance(b, list)) and (\n",
    "                not isinstance(b, np.ndarray))) else b for b in batch]\n",
    "        print(batch[12])\n",
    "        for i in batch[12]:\n",
    "            for j in i:\n",
    "                print(j)\n",
    "                print(tokenizer.decode(j))\n",
    "                \n",
    "        print()\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "관광\n",
      "[CLS] [SEP] 안녕하세요. 어느 지역에 있는 공원을 안내해드릴까요? ; 지역은 공원있는 곳으로 제가 이동하면 되니까 평점 4점 이상인 곳으로 찾아주세요. [SEP] [SLOT] 관광 경치 좋은 - [NULL] [SLOT] 관광 교육적 - [NULL] [SLOT] 관광 도보 가능 - [NULL] [SLOT] 관광 문화 예술 - [NULL] [SLOT] 관광 역사적 - [NULL] [SLOT] 관광 이름 - [NULL] [SLOT] 관광 종류 - [NULL] [SLOT] 관광 주차 가능 - [NULL] [SLOT] 관광 지역 - [NULL] [SLOT] 숙소 가격대 - [NULL] [SLOT] 숙소 도보 가능 - [NULL] [SLOT] 숙소 수영장 유무 - [NULL] [SLOT] 숙소 스파 유무 - [NULL] [SLOT] 숙소 예약 기간 - [NULL] [SLOT] 숙소 예약 명수 - [NULL] [SLOT] 숙소 예약 요일 - [NULL] [SLOT] 숙소 이름 - [NULL] [SLOT] 숙소 인터넷 가능 - [NULL] [SLOT] 숙소 조식 가능 - [NULL] [SLOT] 숙소 종류 - [NULL] [SLOT] 숙소 주차 가능 - [NULL] [SLOT] 숙소 지역 - [NULL] [SLOT] 숙소 헬스장 유무 - [NULL] [SLOT] 숙소 흡연 가능 - [NULL] [SLOT] 식당 가격대 - [NULL] [SLOT] 식당 도보 가능 - [NULL] [SLOT] 식당 야외석 유무 - [NULL] [SLOT] 식당 예약 명수 - [NULL] [SLOT] 식당 예약 시간 - [NULL] [SLOT] 식당 예약 요일 - [NULL] [SLOT] 식당 이름 - [NULL] [SLOT] 식당 인터넷 가능 - [NULL] [SLOT] 식당 종류 - [NULL] [SLOT] 식당 주류 판매 - [NULL] [SLOT] 식당 주차 가능 - [NULL] [SLOT] 식당 지역 - [NULL] [SLOT] 식당 흡연 가능 - [NULL] [SLOT] 지하철 도착지 - [NULL] [SLOT] 지하철 출발 시간 - [NULL] [SLOT] 지하철 출발지 - [NULL] [SLOT] 택시 도착 시간 - [NULL] [SLOT] 택시 도착지 - [NULL] [SLOT] 택시 종류 - [NULL] [SLOT] 택시 출발 시간 - [NULL] [SLOT] 택시 출발지 - [NULL]\n",
      "{}\n",
      "['관광-종류-공원', '관광-지역-dontcare']\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for idx, i in enumerate(train_data_raw[:20]):\n",
    "    if i.id == \"orange-dream-2065:관광_숙소_식당_11\":\n",
    "        num = idx\n",
    "        break\n",
    "\n",
    "num = idx\n",
    "print(train_data_raw[num].turn_domain)\n",
    "print(tokenizer.decode(train_data_raw[num].input_id_p))\n",
    "print(train_data_raw[num].last_dialog_state)\n",
    "print(train_data_raw[num].gold_state)\n",
    "print(train_data_raw[num].is_last_turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "JKJ4kOck26N4",
    "outputId": "2e7bad92-0e5d-41db-e4db-9e0d0542aea7",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(train_data_raw)):\n",
    "    if train_data_raw[i].is_last_turn and ['-'.join(x.split(\"-\")[0:2]) for x in train_data_raw[i].gold_state] != [x for x in train_data_raw[i].last_dialog_state.keys()]:\n",
    "#     if train_data_raw[i].is_last_turn:\n",
    "#         print(train_data_raw[i].turn_domain)\n",
    "        print(tokenizer.decode(train_data_raw[i].input_id_p))\n",
    "#         print(train_data_raw[i].last_dialog_state)\n",
    "#         print({'-'.join(x.split(\"-\")[0:2]) : x.split(\"-\")[-1] for x in train_data_raw[i].gold_state})\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 주류는 판매하고 있고 주차도 가능합니다. 더 궁금하신 점 있으신가요? ; 아니용 [SEP] 감사합니다. 즐거운 여행되세요. ; [SEP] [SLOT] 관광 경치 좋은 - [NULL] [SLOT] 관광 교육적 - [NULL] [SLOT] 관광 도보 가능 - [NULL] [SLOT] 관광 문화 예술 - [NULL] [SLOT] 관광 역사적 - [NULL] [SLOT] 관광 이름 - 문화역서울 284 [SLOT] 관광 종류 - 박물관 [SLOT] 관광 주차 가능 - [NULL] [SLOT] 관광 지역 - 서울 중앙 [SLOT] 숙소 가격대 - [NULL] [SLOT] 숙소 도보 가능 - [NULL] [SLOT] 숙소 수영장 유무 - [NULL] [SLOT] 숙소 스파 유무 - [NULL] [SLOT] 숙소 예약 기간 - [NULL] [SLOT] 숙소 예약 명수 - [NULL] [SLOT] 숙소 예약 요일 - [NULL] [SLOT] 숙소 이름 - [NULL] [SLOT] 숙소 인터넷 가능 - [NULL] [SLOT] 숙소 조식 가능 - [NULL] [SLOT] 숙소 종류 - [NULL] [SLOT] 숙소 주차 가능 - [NULL] [SLOT] 숙소 지역 - [NULL] [SLOT] 숙소 헬스장 유무 - [NULL] [SLOT] 숙소 흡연 가능 - [NULL] [SLOT] 식당 가격대 - 비싼 [SLOT] 식당 도보 가능 - [NULL] [SLOT] 식당 야외석 유무 - yes [SLOT] 식당 예약 명수 - 1 [SLOT] 식당 예약 시간 - 18 : 00 [SLOT] 식당 예약 요일 - 토요일 [SLOT] 식당 이름 - 외계인의맛집 [SLOT] 식당 인터넷 가능 - [NULL] [SLOT] 식당 종류 - 한식당 [SLOT] 식당 주류 판매 - [NULL] [SLOT] 식당 주차 가능 - [NULL] [SLOT] 식당 지역 - 서울 중앙 [SLOT] 식당 흡연 가능 - [NULL] [SLOT] 지하철 도착지 - [NULL] [SLOT] 지하철 출발 시간 - [NULL] [SLOT] 지하철 출발지 - [NULL] [SLOT] 택시 도착 시간 - [NULL] [SLOT] 택시 도착지 - [NULL] [SLOT] 택시 종류 - [NULL] [SLOT] 택시 출발 시간 - [NULL] [SLOT] 택시 출발지 - [NULL]\n",
      "['관광-종류-박물관', '관광-지역-서울 중앙', '관광-이름-문화역서울 284', '식당-가격대-비싼', '식당-지역-서울 중앙', '식당-종류-한식당', '식당-야외석 유무-yes', '식당-예약 요일-토요일', '식당-예약 시간-18:00', '식당-예약 명수-1', '식당-이름-외계인의맛집']\n",
      "\n",
      "[CLS] 그럼. 연락처는 6182006591이고 평점은 4점입니다. ; 와 감사합니다. [SEP] 감사합니다. ; [SEP] [SLOT] 관광 경치 좋은 - [NULL] [SLOT] 관광 교육적 - [NULL] [SLOT] 관광 도보 가능 - [NULL] [SLOT] 관광 문화 예술 - [NULL] [SLOT] 관광 역사적 - [NULL] [SLOT] 관광 이름 - 노량진 수산물 도매시장 [SLOT] 관광 종류 - 쇼핑 [SLOT] 관광 주차 가능 - [NULL] [SLOT] 관광 지역 - 서울 서쪽 [SLOT] 숙소 가격대 - [NULL] [SLOT] 숙소 도보 가능 - [NULL] [SLOT] 숙소 수영장 유무 - [NULL] [SLOT] 숙소 스파 유무 - [NULL] [SLOT] 숙소 예약 기간 - [NULL] [SLOT] 숙소 예약 명수 - [NULL] [SLOT] 숙소 예약 요일 - [NULL] [SLOT] 숙소 이름 - [NULL] [SLOT] 숙소 인터넷 가능 - [NULL] [SLOT] 숙소 조식 가능 - [NULL] [SLOT] 숙소 종류 - [NULL] [SLOT] 숙소 주차 가능 - [NULL] [SLOT] 숙소 지역 - [NULL] [SLOT] 숙소 헬스장 유무 - [NULL] [SLOT] 숙소 흡연 가능 - [NULL] [SLOT] 식당 가격대 - [NULL] [SLOT] 식당 도보 가능 - [NULL] [SLOT] 식당 야외석 유무 - [NULL] [SLOT] 식당 예약 명수 - [NULL] [SLOT] 식당 예약 시간 - [NULL] [SLOT] 식당 예약 요일 - [NULL] [SLOT] 식당 이름 - [NULL] [SLOT] 식당 인터넷 가능 - [NULL] [SLOT] 식당 종류 - [NULL] [SLOT] 식당 주류 판매 - [NULL] [SLOT] 식당 주차 가능 - [NULL] [SLOT] 식당 지역 - [NULL] [SLOT] 식당 흡연 가능 - [NULL] [SLOT] 지하철 도착지 - [NULL] [SLOT] 지하철 출발 시간 - [NULL] [SLOT] 지하철 출발지 - [NULL] [SLOT] 택시 도착 시간 - [NULL] [SLOT] 택시 도착지 - [NULL] [SLOT] 택시 종류 - [NULL] [SLOT] 택시 출발 시간 - [NULL] [SLOT] 택시 출발지 - [NULL]\n",
      "['관광-종류-쇼핑', '관광-지역-서울 서쪽', '관광-이름-노량진 수산물 도매시장']\n",
      "\n",
      "[CLS] 동대문역은 1호선, 4호선으로 확인됩니다. 추가 질문이 있으신가요? ; 아니예요. 확인 감사합니다. 그럼 수고하세요. [SEP] 네. 문의 사항 있으시면 연락주세요. 감사합니다. ; [SEP] [SLOT] 관광 경치 좋은 - [NULL] [SLOT] 관광 교육적 - [NULL] [SLOT] 관광 도보 가능 - [NULL] [SLOT] 관광 문화 예술 - [NULL] [SLOT] 관광 역사적 - [NULL] [SLOT] 관광 이름 - 서울 대학로 [SLOT] 관광 종류 - 기타 [SLOT] 관광 주차 가능 - [NULL] [SLOT] 관광 지역 - 서울 북쪽 [SLOT] 숙소 가격대 - [NULL] [SLOT] 숙소 도보 가능 - [NULL] [SLOT] 숙소 수영장 유무 - [NULL] [SLOT] 숙소 스파 유무 - [NULL] [SLOT] 숙소 예약 기간 - [NULL] [SLOT] 숙소 예약 명수 - [NULL] [SLOT] 숙소 예약 요일 - [NULL] [SLOT] 숙소 이름 - [NULL] [SLOT] 숙소 인터넷 가능 - [NULL] [SLOT] 숙소 조식 가능 - [NULL] [SLOT] 숙소 종류 - [NULL] [SLOT] 숙소 주차 가능 - [NULL] [SLOT] 숙소 지역 - [NULL] [SLOT] 숙소 헬스장 유무 - [NULL] [SLOT] 숙소 흡연 가능 - [NULL] [SLOT] 식당 가격대 - 적당 [SLOT] 식당 도보 가능 - [NULL] [SLOT] 식당 야외석 유무 - [NULL] [SLOT] 식당 예약 명수 - 4 [SLOT] 식당 예약 시간 - 03 : 30 [SLOT] 식당 예약 요일 - 금요일 [SLOT] 식당 이름 - 어차피자 [SLOT] 식당 인터넷 가능 - [NULL] [SLOT] 식당 종류 - 양식당 [SLOT] 식당 주류 판매 - [NULL] [SLOT] 식당 주차 가능 - [NULL] [SLOT] 식당 지역 - dontcare [SLOT] 식당 흡연 가능 - [NULL] [SLOT] 지하철 도착지 - 동대문역 [SLOT] 지하철 출발 시간 - [NULL] [SLOT] 지하철 출발지 - 혜화역 [SLOT] 택시 도착 시간 - [NULL] [SLOT] 택시 도착지 - [NULL] [SLOT] 택시 종류 - [NULL] [SLOT] 택시 출발 시간 - [NULL] [SLOT] 택시 출발지 - [NULL]\n",
      "['지하철-출발지-혜화역', '지하철-도착지-동대문역', '관광-종류-기타', '관광-지역-서울 북쪽', '관광-이름-서울 대학로', '식당-가격대-적당', '식당-지역-dontcare', '식당-종류-양식당', '식당-예약 요일-금요일', '식당-예약 시간-03:30', '식당-예약 명수-4', '식당-이름-어차피자']\n",
      "\n",
      "[CLS] 주소는 서울 종로구 07645입니다. 더 궁금한건 없으세요? ; 없습니다. 감사합니다. [SEP] 네. 이용해 주셔서 감사합니다. ; [SEP] [SLOT] 관광 경치 좋은 - [NULL] [SLOT] 관광 교육적 - [NULL] [SLOT] 관광 도보 가능 - [NULL] [SLOT] 관광 문화 예술 - [NULL] [SLOT] 관광 역사적 - [NULL] [SLOT] 관광 이름 - 서울로7017 [SLOT] 관광 종류 - 공원 [SLOT] 관광 주차 가능 - [NULL] [SLOT] 관광 지역 - dontcare [SLOT] 숙소 가격대 - [NULL] [SLOT] 숙소 도보 가능 - [NULL] [SLOT] 숙소 수영장 유무 - [NULL] [SLOT] 숙소 스파 유무 - [NULL] [SLOT] 숙소 예약 기간 - [NULL] [SLOT] 숙소 예약 명수 - [NULL] [SLOT] 숙소 예약 요일 - [NULL] [SLOT] 숙소 이름 - [NULL] [SLOT] 숙소 인터넷 가능 - [NULL] [SLOT] 숙소 조식 가능 - [NULL] [SLOT] 숙소 종류 - [NULL] [SLOT] 숙소 주차 가능 - [NULL] [SLOT] 숙소 지역 - [NULL] [SLOT] 숙소 헬스장 유무 - [NULL] [SLOT] 숙소 흡연 가능 - [NULL] [SLOT] 식당 가격대 - dontcare [SLOT] 식당 도보 가능 - [NULL] [SLOT] 식당 야외석 유무 - [NULL] [SLOT] 식당 예약 명수 - 3 [SLOT] 식당 예약 시간 - 12 : 30 [SLOT] 식당 예약 요일 - 수요일 [SLOT] 식당 이름 - 내맘속의나폴리 [SLOT] 식당 인터넷 가능 - yes [SLOT] 식당 종류 - 양식당 [SLOT] 식당 주류 판매 - [NULL] [SLOT] 식당 주차 가능 - [NULL] [SLOT] 식당 지역 - dontcare [SLOT] 식당 흡연 가능 - [NULL] [SLOT] 지하철 도착지 - [NULL] [SLOT] 지하철 출발 시간 - [NULL] [SLOT] 지하철 출발지 - [NULL] [SLOT] 택시 도착 시간 - [NULL] [SLOT] 택시 도착지 - [NULL] [SLOT] 택시 종류 - [NULL] [SLOT] 택시 출발 시간 - [NULL] [SLOT] 택시 출발지 - [NULL]\n",
      "['관광-종류-공원', '관광-지역-dontcare', '관광-이름-서울로7017', '식당-가격대-dontcare', '식당-지역-dontcare', '식당-종류-양식당', '식당-인터넷 가능-yes', '식당-예약 요일-수요일', '식당-예약 시간-12:30', '식당-예약 명수-3', '식당-이름-내맘속의나폴리']\n",
      "\n",
      "[CLS] 10000원으로 이용하실 수 있는 고급택시가 검색되었습니다. 전화번호는 06837405219 입니다. 더 궁금하신 사항 있으신가요? ; 아니요 감사합니다. [SEP] 감사합니다. ; [SEP] [SLOT] 관광 경치 좋은 - [NULL] [SLOT] 관광 교육적 - [NULL] [SLOT] 관광 도보 가능 - [NULL] [SLOT] 관광 문화 예술 - [NULL] [SLOT] 관광 역사적 - [NULL] [SLOT] 관광 이름 - [NULL] [SLOT] 관광 종류 - [NULL] [SLOT] 관광 주차 가능 - [NULL] [SLOT] 관광 지역 - [NULL] [SLOT] 숙소 가격대 - [NULL] [SLOT] 숙소 도보 가능 - [NULL] [SLOT] 숙소 수영장 유무 - [NULL] [SLOT] 숙소 스파 유무 - [NULL] [SLOT] 숙소 예약 기간 - [NULL] [SLOT] 숙소 예약 명수 - [NULL] [SLOT] 숙소 예약 요일 - [NULL] [SLOT] 숙소 이름 - [NULL] [SLOT] 숙소 인터넷 가능 - [NULL] [SLOT] 숙소 조식 가능 - [NULL] [SLOT] 숙소 종류 - [NULL] [SLOT] 숙소 주차 가능 - [NULL] [SLOT] 숙소 지역 - [NULL] [SLOT] 숙소 헬스장 유무 - [NULL] [SLOT] 숙소 흡연 가능 - [NULL] [SLOT] 식당 가격대 - [NULL] [SLOT] 식당 도보 가능 - [NULL] [SLOT] 식당 야외석 유무 - [NULL] [SLOT] 식당 예약 명수 - [NULL] [SLOT] 식당 예약 시간 - [NULL] [SLOT] 식당 예약 요일 - [NULL] [SLOT] 식당 이름 - [NULL] [SLOT] 식당 인터넷 가능 - [NULL] [SLOT] 식당 종류 - [NULL] [SLOT] 식당 주류 판매 - [NULL] [SLOT] 식당 주차 가능 - [NULL] [SLOT] 식당 지역 - [NULL] [SLOT] 식당 흡연 가능 - [NULL] [SLOT] 지하철 도착지 - [NULL] [SLOT] 지하철 출발 시간 - [NULL] [SLOT] 지하철 출발지 - [NULL] [SLOT] 택시 도착 시간 - 13 : 20 [SLOT] 택시 도착지 - 모텔 킹 [SLOT] 택시 종류 - 고급 [SLOT] 택시 출발 시간 - dontcare [SLOT] 택시 출발지 - 인사동\n",
      "['택시-출발 시간-dontcare', '택시-출발지-인사동', '택시-도착지-모텔 킹', '택시-도착 시간-13:20', '택시-종류-고급']\n",
      "\n",
      "[CLS] 모두 숙박하시기 48시간 전까지 연락주셔야 합니다. ; 아 그렇군요. 알겠습니다. 그럼 좋은 하루되세요. [SEP] 네. 이용해주셔서 감사합니다. ; [SEP] [SLOT] 관광 경치 좋은 - [NULL] [SLOT] 관광 교육적 - [NULL] [SLOT] 관광 도보 가능 - [NULL] [SLOT] 관광 문화 예술 - [NULL] [SLOT] 관광 역사적 - [NULL] [SLOT] 관광 이름 - [NULL] [SLOT] 관광 종류 - [NULL] [SLOT] 관광 주차 가능 - [NULL] [SLOT] 관광 지역 - [NULL] [SLOT] 숙소 가격대 - dontcare [SLOT] 숙소 도보 가능 - [NULL] [SLOT] 숙소 수영장 유무 - [NULL] [SLOT] 숙소 스파 유무 - [NULL] [SLOT] 숙소 예약 기간 - 3 [SLOT] 숙소 예약 명수 - 1 [SLOT] 숙소 예약 요일 - 수요일 [SLOT] 숙소 이름 - 야드 서울 [SLOT] 숙소 인터넷 가능 - yes [SLOT] 숙소 조식 가능 - [NULL] [SLOT] 숙소 종류 - 모텔 [SLOT] 숙소 주차 가능 - [NULL] [SLOT] 숙소 지역 - 서울 중앙 [SLOT] 숙소 헬스장 유무 - [NULL] [SLOT] 숙소 흡연 가능 - yes [SLOT] 식당 가격대 - [NULL] [SLOT] 식당 도보 가능 - [NULL] [SLOT] 식당 야외석 유무 - [NULL] [SLOT] 식당 예약 명수 - [NULL] [SLOT] 식당 예약 시간 - [NULL] [SLOT] 식당 예약 요일 - [NULL] [SLOT] 식당 이름 - [NULL] [SLOT] 식당 인터넷 가능 - [NULL] [SLOT] 식당 종류 - [NULL] [SLOT] 식당 주류 판매 - [NULL] [SLOT] 식당 주차 가능 - [NULL] [SLOT] 식당 지역 - [NULL] [SLOT] 식당 흡연 가능 - [NULL] [SLOT] 지하철 도착지 - [NULL] [SLOT] 지하철 출발 시간 - [NULL] [SLOT] 지하철 출발지 - [NULL] [SLOT] 택시 도착 시간 - [NULL] [SLOT] 택시 도착지 - 회오리라멘 [SLOT] 택시 종류 - dontcare [SLOT] 택시 출발 시간 - 10 : 00 [SLOT] 택시 출발지 - 야드 서울\n",
      "['숙소-가격대-dontcare', '숙소-종류-모텔', '숙소-지역-서울 중앙', '숙소-인터넷 가능-yes', '숙소-흡연 가능-yes', '숙소-예약 요일-수요일', '숙소-예약 명수-1', '숙소-예약 기간-3', '숙소-이름-야드 서울', '택시-출발 시간-10:00', '택시-출발지-야드 서울', '택시-도착지-회오리라멘', '택시-종류-dontcare']\n",
      "\n",
      "[CLS] 더 궁금하신 사항은 없으신가요? ; 네 [SEP] 예약 감사합니다. 즐거운 하루 되세요. ; [SEP] [SLOT] 관광 경치 좋은 - [NULL] [SLOT] 관광 교육적 - [NULL] [SLOT] 관광 도보 가능 - [NULL] [SLOT] 관광 문화 예술 - [NULL] [SLOT] 관광 역사적 - [NULL] [SLOT] 관광 이름 - [NULL] [SLOT] 관광 종류 - [NULL] [SLOT] 관광 주차 가능 - [NULL] [SLOT] 관광 지역 - [NULL] [SLOT] 숙소 가격대 - [NULL] [SLOT] 숙소 도보 가능 - [NULL] [SLOT] 숙소 수영장 유무 - [NULL] [SLOT] 숙소 스파 유무 - [NULL] [SLOT] 숙소 예약 기간 - [NULL] [SLOT] 숙소 예약 명수 - [NULL] [SLOT] 숙소 예약 요일 - [NULL] [SLOT] 숙소 이름 - [NULL] [SLOT] 숙소 인터넷 가능 - [NULL] [SLOT] 숙소 조식 가능 - [NULL] [SLOT] 숙소 종류 - [NULL] [SLOT] 숙소 주차 가능 - [NULL] [SLOT] 숙소 지역 - [NULL] [SLOT] 숙소 헬스장 유무 - [NULL] [SLOT] 숙소 흡연 가능 - [NULL] [SLOT] 식당 가격대 - dontcare [SLOT] 식당 도보 가능 - [NULL] [SLOT] 식당 야외석 유무 - yes [SLOT] 식당 예약 명수 - 8 [SLOT] 식당 예약 시간 - 19 : 30 [SLOT] 식당 예약 요일 - 금요일 [SLOT] 식당 이름 - 피렌체의밤 [SLOT] 식당 인터넷 가능 - [NULL] [SLOT] 식당 종류 - 양식당 [SLOT] 식당 주류 판매 - [NULL] [SLOT] 식당 주차 가능 - yes [SLOT] 식당 지역 - 서울 서쪽 [SLOT] 식당 흡연 가능 - [NULL] [SLOT] 지하철 도착지 - [NULL] [SLOT] 지하철 출발 시간 - [NULL] [SLOT] 지하철 출발지 - [NULL] [SLOT] 택시 도착 시간 - [NULL] [SLOT] 택시 도착지 - [NULL] [SLOT] 택시 종류 - [NULL] [SLOT] 택시 출발 시간 - [NULL] [SLOT] 택시 출발지 - [NULL]\n",
      "['식당-가격대-dontcare', '식당-지역-서울 서쪽', '식당-종류-양식당', '식당-주차 가능-yes', '식당-야외석 유무-yes', '식당-예약 요일-금요일', '식당-예약 시간-19:30', '식당-예약 명수-8', '식당-이름-피렌체의밤']\n",
      "\n",
      "[CLS] 택시 기사님 전화번호는 08037659124이고 배정된 택시의 종류는 에코 택시로 확인 됩니다. 추가로 질문 있으신가요? ; 아니요. 완전 괜찮습니다. 제가 시간을 너무 많이 뺏은거 같아서 죄송스럽네요.. [SEP] 아닙니다. 저도 즐거웠는데요. 아무쪼록 즐거운 여정 보내시고 다음에 또 이용 부탁 드립니다. 감사합니다. ; [SEP] [SLOT] 관광 경치 좋은 - [NULL] [SLOT] 관광 교육적 - [NULL] [SLOT] 관광 도보 가능 - [NULL] [SLOT] 관광 문화 예술 - [NULL] [SLOT] 관광 역사적 - [NULL] [SLOT] 관광 이름 - [NULL] [SLOT] 관광 종류 - [NULL] [SLOT] 관광 주차 가능 - [NULL] [SLOT] 관광 지역 - [NULL] [SLOT] 숙소 가격대 - 적당 [SLOT] 숙소 도보 가능 - [NULL] [SLOT] 숙소 수영장 유무 - [NULL] [SLOT] 숙소 스파 유무 - yes [SLOT] 숙소 예약 기간 - 3 [SLOT] 숙소 예약 명수 - 2 [SLOT] 숙소 예약 요일 - 일요일 [SLOT] 숙소 이름 - 포레스트 호텔 [SLOT] 숙소 인터넷 가능 - [NULL] [SLOT] 숙소 조식 가능 - [NULL] [SLOT] 숙소 종류 - dontcare [SLOT] 숙소 주차 가능 - [NULL] [SLOT] 숙소 지역 - 서울 남쪽 [SLOT] 숙소 헬스장 유무 - [NULL] [SLOT] 숙소 흡연 가능 - [NULL] [SLOT] 식당 가격대 - [NULL] [SLOT] 식당 도보 가능 - [NULL] [SLOT] 식당 야외석 유무 - [NULL] [SLOT] 식당 예약 명수 - [NULL] [SLOT] 식당 예약 시간 - [NULL] [SLOT] 식당 예약 요일 - [NULL] [SLOT] 식당 이름 - [NULL] [SLOT] 식당 인터넷 가능 - [NULL] [SLOT] 식당 종류 - [NULL] [SLOT] 식당 주류 판매 - [NULL] [SLOT] 식당 주차 가능 - [NULL] [SLOT] 식당 지역 - [NULL] [SLOT] 식당 흡연 가능 - [NULL] [SLOT] 지하철 도착지 - [NULL] [SLOT] 지하철 출발 시간 - [NULL] [SLOT] 지하철 출발지 - [NULL] [SLOT] 택시 도착 시간 - dontcare [SLOT] 택시 도착지 - 포레스트 호텔 [SLOT] 택시 종류 - dontcare [SLOT] 택시 출발 시간 - dontcare [SLOT] 택시 출발지 - 분식회개\n",
      "['숙소-가격대-적당', '숙소-종류-dontcare', '숙소-지역-서울 남쪽', '숙소-스파 유무-yes', '숙소-예약 요일-일요일', '숙소-예약 명수-2', '숙소-예약 기간-3', '숙소-이름-포레스트 호텔', '택시-출발 시간-dontcare', '택시-출발지-분식회개', '택시-도착지-포레스트 호텔', '택시-도착 시간-dontcare', '택시-종류-dontcare']\n",
      "\n",
      "[CLS] 더 궁금하신 사항은 없으신가요? ; 넵 [SEP] 네 알겠습니다. 이용 감사합니다. 좋은 하루 되세요. ; [SEP] [SLOT] 관광 경치 좋은 - [NULL] [SLOT] 관광 교육적 - [NULL] [SLOT] 관광 도보 가능 - [NULL] [SLOT] 관광 문화 예술 - [NULL] [SLOT] 관광 역사적 - [NULL] [SLOT] 관광 이름 - [NULL] [SLOT] 관광 종류 - [NULL] [SLOT] 관광 주차 가능 - [NULL] [SLOT] 관광 지역 - [NULL] [SLOT] 숙소 가격대 - 저렴 [SLOT] 숙소 도보 가능 - [NULL] [SLOT] 숙소 수영장 유무 - [NULL] [SLOT] 숙소 스파 유무 - [NULL] [SLOT] 숙소 예약 기간 - 3 [SLOT] 숙소 예약 명수 - 1 [SLOT] 숙소 예약 요일 - 금요일 [SLOT] 숙소 이름 - 서촌 한옥 게스트하우스 [SLOT] 숙소 인터넷 가능 - [NULL] [SLOT] 숙소 조식 가능 - [NULL] [SLOT] 숙소 종류 - dontcare [SLOT] 숙소 주차 가능 - [NULL] [SLOT] 숙소 지역 - 서울 중앙 [SLOT] 숙소 헬스장 유무 - [NULL] [SLOT] 숙소 흡연 가능 - [NULL] [SLOT] 식당 가격대 - dontcare [SLOT] 식당 도보 가능 - [NULL] [SLOT] 식당 야외석 유무 - [NULL] [SLOT] 식당 예약 명수 - 1 [SLOT] 식당 예약 시간 - 17 : 00 [SLOT] 식당 예약 요일 - 금요일 [SLOT] 식당 이름 - 믿고맡기는초밥 [SLOT] 식당 인터넷 가능 - [NULL] [SLOT] 식당 종류 - 일식당 [SLOT] 식당 주류 판매 - [NULL] [SLOT] 식당 주차 가능 - [NULL] [SLOT] 식당 지역 - 서울 중앙 [SLOT] 식당 흡연 가능 - [NULL] [SLOT] 지하철 도착지 - [NULL] [SLOT] 지하철 출발 시간 - [NULL] [SLOT] 지하철 출발지 - [NULL] [SLOT] 택시 도착 시간 - [NULL] [SLOT] 택시 도착지 - [NULL] [SLOT] 택시 종류 - [NULL] [SLOT] 택시 출발 시간 - [NULL] [SLOT] 택시 출발지 - [NULL]\n",
      "['숙소-가격대-저렴', '숙소-종류-dontcare', '숙소-지역-서울 중앙', '숙소-예약 요일-금요일', '숙소-예약 명수-1', '숙소-예약 기간-3', '숙소-이름-서촌 한옥 게스트하우스', '식당-가격대-dontcare', '식당-지역-서울 중앙', '식당-종류-일식당', '식당-예약 요일-금요일', '식당-예약 시간-17:00', '식당-예약 명수-1', '식당-이름-믿고맡기는초밥']\n",
      "\n",
      "[CLS] 다른 요청 사항은 없으십니까? ; 네 [SEP] 네 이용 감사합니다. 좋은 하루 되세요. ; [SEP] [SLOT] 관광 경치 좋은 - [NULL] [SLOT] 관광 교육적 - [NULL] [SLOT] 관광 도보 가능 - [NULL] [SLOT] 관광 문화 예술 - [NULL] [SLOT] 관광 역사적 - [NULL] [SLOT] 관광 이름 - [NULL] [SLOT] 관광 종류 - [NULL] [SLOT] 관광 주차 가능 - [NULL] [SLOT] 관광 지역 - [NULL] [SLOT] 숙소 가격대 - 적당 [SLOT] 숙소 도보 가능 - [NULL] [SLOT] 숙소 수영장 유무 - [NULL] [SLOT] 숙소 스파 유무 - yes [SLOT] 숙소 예약 기간 - 1 [SLOT] 숙소 예약 명수 - 2 [SLOT] 숙소 예약 요일 - 월요일 [SLOT] 숙소 이름 - 패밀리 호텔 [SLOT] 숙소 인터넷 가능 - [NULL] [SLOT] 숙소 조식 가능 - [NULL] [SLOT] 숙소 종류 - dontcare [SLOT] 숙소 주차 가능 - [NULL] [SLOT] 숙소 지역 - 서울 남쪽 [SLOT] 숙소 헬스장 유무 - [NULL] [SLOT] 숙소 흡연 가능 - [NULL] [SLOT] 식당 가격대 - [NULL] [SLOT] 식당 도보 가능 - [NULL] [SLOT] 식당 야외석 유무 - [NULL] [SLOT] 식당 예약 명수 - [NULL] [SLOT] 식당 예약 시간 - [NULL] [SLOT] 식당 예약 요일 - [NULL] [SLOT] 식당 이름 - [NULL] [SLOT] 식당 인터넷 가능 - [NULL] [SLOT] 식당 종류 - [NULL] [SLOT] 식당 주류 판매 - [NULL] [SLOT] 식당 주차 가능 - [NULL] [SLOT] 식당 지역 - [NULL] [SLOT] 식당 흡연 가능 - [NULL] [SLOT] 지하철 도착지 - [NULL] [SLOT] 지하철 출발 시간 - [NULL] [SLOT] 지하철 출발지 - [NULL] [SLOT] 택시 도착 시간 - 04 : 50 [SLOT] 택시 도착지 - 패밀리 호텔 [SLOT] 택시 종류 - dontcare [SLOT] 택시 출발 시간 - dontcare [SLOT] 택시 출발지 - 잠실역\n",
      "['숙소-가격대-적당', '숙소-종류-dontcare', '숙소-지역-서울 남쪽', '숙소-스파 유무-yes', '숙소-예약 요일-월요일', '숙소-예약 명수-2', '숙소-예약 기간-1', '숙소-이름-패밀리 호텔', '택시-출발 시간-dontcare', '택시-출발지-잠실역', '택시-도착지-패밀리 호텔', '택시-도착 시간-04:50', '택시-종류-dontcare']\n",
      "\n",
      "[CLS] 입장료는 9000원입니다. 제가 더 도울 일이 있을까요? ; 아니용 [SEP] 감사합니다. 즐거운 여행되세요. ; [SEP] [SLOT] 관광 경치 좋은 - [NULL] [SLOT] 관광 교육적 - [NULL] [SLOT] 관광 도보 가능 - [NULL] [SLOT] 관광 문화 예술 - [NULL] [SLOT] 관광 역사적 - no [SLOT] 관광 이름 - 광장시장 [SLOT] 관광 종류 - 쇼핑 [SLOT] 관광 주차 가능 - [NULL] [SLOT] 관광 지역 - 서울 북쪽 [SLOT] 숙소 가격대 - [NULL] [SLOT] 숙소 도보 가능 - [NULL] [SLOT] 숙소 수영장 유무 - [NULL] [SLOT] 숙소 스파 유무 - [NULL] [SLOT] 숙소 예약 기간 - [NULL] [SLOT] 숙소 예약 명수 - [NULL] [SLOT] 숙소 예약 요일 - [NULL] [SLOT] 숙소 이름 - [NULL] [SLOT] 숙소 인터넷 가능 - [NULL] [SLOT] 숙소 조식 가능 - [NULL] [SLOT] 숙소 종류 - [NULL] [SLOT] 숙소 주차 가능 - [NULL] [SLOT] 숙소 지역 - [NULL] [SLOT] 숙소 헬스장 유무 - [NULL] [SLOT] 숙소 흡연 가능 - [NULL] [SLOT] 식당 가격대 - 비싼 [SLOT] 식당 도보 가능 - [NULL] [SLOT] 식당 야외석 유무 - [NULL] [SLOT] 식당 예약 명수 - 1 [SLOT] 식당 예약 시간 - 12 : 45 [SLOT] 식당 예약 요일 - 목요일 [SLOT] 식당 이름 - 식객 [SLOT] 식당 인터넷 가능 - [NULL] [SLOT] 식당 종류 - 한식당 [SLOT] 식당 주류 판매 - yes [SLOT] 식당 주차 가능 - [NULL] [SLOT] 식당 지역 - dontcare [SLOT] 식당 흡연 가능 - [NULL] [SLOT] 지하철 도착지 - [NULL] [SLOT] 지하철 출발 시간 - [NULL] [SLOT] 지하철 출발지 - [NULL] [SLOT] 택시 도착 시간 - [NULL] [SLOT] 택시 도착지 - [NULL] [SLOT] 택시 종류 - [NULL] [SLOT] 택시 출발 시간 - [NULL] [SLOT] 택시 출발지 - [NULL]\n",
      "['관광-종류-쇼핑', '관광-지역-서울 북쪽', '관광-역사적-no', '관광-이름-광장시장', '식당-가격대-비싼', '식당-지역-dontcare', '식당-종류-한식당', '식당-주류 판매-yes', '식당-예약 요일-목요일', '식당-예약 시간-12:45', '식당-예약 명수-1', '식당-이름-식객']\n",
      "\n",
      "[CLS] 180분 소요됩니다. 궁금증이 다 해결되셨나요? ; 네 감사합니다. [SEP] 네 감사합니다. 좋은 시간 보내세요 ; [SEP] [SLOT] 관광 경치 좋은 - [NULL] [SLOT] 관광 교육적 - [NULL] [SLOT] 관광 도보 가능 - [NULL] [SLOT] 관광 문화 예술 - [NULL] [SLOT] 관광 역사적 - [NULL] [SLOT] 관광 이름 - [NULL] [SLOT] 관광 종류 - [NULL] [SLOT] 관광 주차 가능 - [NULL] [SLOT] 관광 지역 - [NULL] [SLOT] 숙소 가격대 - [NULL] [SLOT] 숙소 도보 가능 - [NULL] [SLOT] 숙소 수영장 유무 - [NULL] [SLOT] 숙소 스파 유무 - [NULL] [SLOT] 숙소 예약 기간 - [NULL] [SLOT] 숙소 예약 명수 - [NULL] [SLOT] 숙소 예약 요일 - [NULL] [SLOT] 숙소 이름 - [NULL] [SLOT] 숙소 인터넷 가능 - [NULL] [SLOT] 숙소 조식 가능 - [NULL] [SLOT] 숙소 종류 - [NULL] [SLOT] 숙소 주차 가능 - [NULL] [SLOT] 숙소 지역 - [NULL] [SLOT] 숙소 헬스장 유무 - [NULL] [SLOT] 숙소 흡연 가능 - [NULL] [SLOT] 식당 가격대 - 적당 [SLOT] 식당 도보 가능 - [NULL] [SLOT] 식당 야외석 유무 - no [SLOT] 식당 예약 명수 - 4 [SLOT] 식당 예약 시간 - 19 : 30 [SLOT] 식당 예약 요일 - 토요일 [SLOT] 식당 이름 - 부찌부찌 [SLOT] 식당 인터넷 가능 - [NULL] [SLOT] 식당 종류 - 한식당 [SLOT] 식당 주류 판매 - [NULL] [SLOT] 식당 주차 가능 - [NULL] [SLOT] 식당 지역 - 서울 동쪽 [SLOT] 식당 흡연 가능 - [NULL] [SLOT] 지하철 도착지 - [NULL] [SLOT] 지하철 출발 시간 - [NULL] [SLOT] 지하철 출발지 - [NULL] [SLOT] 택시 도착 시간 - [NULL] [SLOT] 택시 도착지 - 노원역 [SLOT] 택시 종류 - dontcare [SLOT] 택시 출발 시간 - 13 : 00 [SLOT] 택시 출발지 - 왕십리역\n",
      "['식당-가격대-적당', '식당-지역-서울 동쪽', '식당-종류-한식당', '식당-야외석 유무-no', '식당-예약 요일-토요일', '식당-예약 시간-19:30', '식당-예약 명수-4', '식당-이름-부찌부찌', '택시-출발 시간-13:00', '택시-출발지-왕십리역', '택시-도착지-노원역', '택시-종류-dontcare']\n",
      "\n",
      "[CLS] 요금은 10000원 입니다. 추가 문의사항 있으실까요? ; 없어요. [SEP] 감사합니다. ; [SEP] [SLOT] 관광 경치 좋은 - [NULL] [SLOT] 관광 교육적 - [NULL] [SLOT] 관광 도보 가능 - [NULL] [SLOT] 관광 문화 예술 - [NULL] [SLOT] 관광 역사적 - [NULL] [SLOT] 관광 이름 - [NULL] [SLOT] 관광 종류 - [NULL] [SLOT] 관광 주차 가능 - [NULL] [SLOT] 관광 지역 - [NULL] [SLOT] 숙소 가격대 - [NULL] [SLOT] 숙소 도보 가능 - [NULL] [SLOT] 숙소 수영장 유무 - [NULL] [SLOT] 숙소 스파 유무 - [NULL] [SLOT] 숙소 예약 기간 - [NULL] [SLOT] 숙소 예약 명수 - [NULL] [SLOT] 숙소 예약 요일 - [NULL] [SLOT] 숙소 이름 - [NULL] [SLOT] 숙소 인터넷 가능 - [NULL] [SLOT] 숙소 조식 가능 - [NULL] [SLOT] 숙소 종류 - [NULL] [SLOT] 숙소 주차 가능 - [NULL] [SLOT] 숙소 지역 - [NULL] [SLOT] 숙소 헬스장 유무 - [NULL] [SLOT] 숙소 흡연 가능 - [NULL] [SLOT] 식당 가격대 - dontcare [SLOT] 식당 도보 가능 - [NULL] [SLOT] 식당 야외석 유무 - [NULL] [SLOT] 식당 예약 명수 - 1 [SLOT] 식당 예약 시간 - 19 : 40 [SLOT] 식당 예약 요일 - 월요일 [SLOT] 식당 이름 - 꼬끼오꼬꼬 [SLOT] 식당 인터넷 가능 - [NULL] [SLOT] 식당 종류 - 한식당 [SLOT] 식당 주류 판매 - [NULL] [SLOT] 식당 주차 가능 - [NULL] [SLOT] 식당 지역 - 서울 중앙 [SLOT] 식당 흡연 가능 - [NULL] [SLOT] 지하철 도착지 - [NULL] [SLOT] 지하철 출발 시간 - [NULL] [SLOT] 지하철 출발지 - [NULL] [SLOT] 택시 도착 시간 - [NULL] [SLOT] 택시 도착지 - 꼬끼오꼬꼬 [SLOT] 택시 종류 - dontcare [SLOT] 택시 출발 시간 - 03 : 00 [SLOT] 택시 출발지 - 종각역\n",
      "['식당-가격대-dontcare', '식당-지역-서울 중앙', '식당-종류-한식당', '식당-예약 요일-월요일', '식당-예약 시간-19:40', '식당-예약 명수-1', '식당-이름-꼬끼오꼬꼬', '택시-출발 시간-03:00', '택시-출발지-종각역', '택시-도착지-꼬끼오꼬꼬', '택시-종류-dontcare']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_data_raw[:100])):\n",
    "#     if train_data_raw[i].is_last_turn == False and train_data_raw[i].turn_utter == []:\n",
    "    if train_data_raw[i].is_last_turn:\n",
    "        print(tokenizer.decode(train_data_raw[i].input_id_p))\n",
    "        print(train_data_raw[i].gold_state)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FF-VWVF_0W3l",
    "outputId": "a94ec477-db46-49a6-87e1-6d7f3256605e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### decoder_teacher_forcing: 1\n",
      "# train examples 57387\n"
     ]
    }
   ],
   "source": [
    "train_data = MultiWozDataset(train_data_raw,\n",
    "                              tokenizer,\n",
    "                              slot_meta,\n",
    "                              args.max_seq_length,\n",
    "                              rng,\n",
    "                              ontology,\n",
    "                              args.word_dropout,\n",
    "                              args.shuffle_state,\n",
    "                              args.shuffle_p, pad_id=tokenizer.convert_tokens_to_ids(['[PAD]'])[0],\n",
    "                              slot_id=tokenizer.convert_tokens_to_ids(['[SLOT]'])[0],\n",
    "                              decoder_teacher_forcing=args.decoder_teacher_forcing,\n",
    "                              use_full_slot=args.use_full_slot,\n",
    "                              use_dt_only=args.use_dt_only, no_dial=args.no_dial,\n",
    "                              use_cls_only=args.use_cls_only)\n",
    "\n",
    "print(\"# train examples %d\" % len(train_data_raw))\n",
    "\n",
    "# if not os.path.exists(dev_path):\n",
    "#     dev_data_raw = prepare_dataset(data_path=args.dev_data_path,\n",
    "#                                     tokenizer=tokenizer,\n",
    "#                                     slot_meta=slot_meta,\n",
    "#                                     n_history=args.n_history,\n",
    "#                                     max_seq_length=args.max_seq_length,\n",
    "#                                     op_code=args.op_code)\n",
    "#     torch.save(dev_data_raw,  dev_path)\n",
    "# else:\n",
    "#     dev_data_raw = torch.load(dev_path)\n",
    "\n",
    "# print(\"# dev examples %d\" % len(dev_data_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sf3Q1FMX6Zn_",
    "outputId": "7f2c9fa0-3a89-4715-b35f-0c74cb61e232",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'ancient-water-1604:식당_8',\n",
       " 'turn_domain': '식당',\n",
       " 'turn_id': 1,\n",
       " 'turn_utter': ['안녕',\n",
       "  '##하',\n",
       "  '##세요',\n",
       "  ',',\n",
       "  '어떤',\n",
       "  '종류',\n",
       "  '##의',\n",
       "  '식당',\n",
       "  '##을',\n",
       "  '찾',\n",
       "  '##으',\n",
       "  '##시',\n",
       "  '##나',\n",
       "  '##요',\n",
       "  '?',\n",
       "  ';',\n",
       "  '종류',\n",
       "  '##요',\n",
       "  '?',\n",
       "  '음',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '종류',\n",
       "  '##는',\n",
       "  '상관없',\n",
       "  '##는데',\n",
       "  '가격',\n",
       "  '##이',\n",
       "  '중요',\n",
       "  '##해',\n",
       "  '##요',\n",
       "  '.',\n",
       "  '여행',\n",
       "  '##중',\n",
       "  '##이다',\n",
       "  '##보',\n",
       "  '##니',\n",
       "  '저렴',\n",
       "  '##한',\n",
       "  '곳',\n",
       "  '##이',\n",
       "  '##었',\n",
       "  '##음',\n",
       "  '해요'],\n",
       " 'dialog_history': [';',\n",
       "  '안녕',\n",
       "  '##하',\n",
       "  '##세요',\n",
       "  ',',\n",
       "  '제',\n",
       "  '##가',\n",
       "  '지금',\n",
       "  '서울',\n",
       "  '북쪽',\n",
       "  '여행',\n",
       "  '##중',\n",
       "  '##인',\n",
       "  '##데',\n",
       "  '이쪽',\n",
       "  '##에',\n",
       "  '있',\n",
       "  '##는',\n",
       "  '식당',\n",
       "  '##을',\n",
       "  '찾',\n",
       "  '##고',\n",
       "  '##있',\n",
       "  '##어요'],\n",
       " 'last_dialog_state': {'식당-지역': '서울 북쪽'},\n",
       " 'gold_p_state': {'식당-지역': '서울 북쪽'},\n",
       " 'generate_y': [['저렴', '[EOS]']],\n",
       " 'op_labels': ['carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'update',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'dontcare',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover',\n",
       "  'carryover'],\n",
       " 'gold_state': ['식당-가격대-저렴', '식당-지역-서울 북쪽', '식당-종류-dontcare'],\n",
       " 'max_seq_length': 512,\n",
       " 'slot_meta': ['관광-경치 좋은',\n",
       "  '관광-교육적',\n",
       "  '관광-도보 가능',\n",
       "  '관광-문화 예술',\n",
       "  '관광-역사적',\n",
       "  '관광-이름',\n",
       "  '관광-종류',\n",
       "  '관광-주차 가능',\n",
       "  '관광-지역',\n",
       "  '숙소-가격대',\n",
       "  '숙소-도보 가능',\n",
       "  '숙소-수영장 유무',\n",
       "  '숙소-스파 유무',\n",
       "  '숙소-예약 기간',\n",
       "  '숙소-예약 명수',\n",
       "  '숙소-예약 요일',\n",
       "  '숙소-이름',\n",
       "  '숙소-인터넷 가능',\n",
       "  '숙소-조식 가능',\n",
       "  '숙소-종류',\n",
       "  '숙소-주차 가능',\n",
       "  '숙소-지역',\n",
       "  '숙소-헬스장 유무',\n",
       "  '숙소-흡연 가능',\n",
       "  '식당-가격대',\n",
       "  '식당-도보 가능',\n",
       "  '식당-야외석 유무',\n",
       "  '식당-예약 명수',\n",
       "  '식당-예약 시간',\n",
       "  '식당-예약 요일',\n",
       "  '식당-이름',\n",
       "  '식당-인터넷 가능',\n",
       "  '식당-종류',\n",
       "  '식당-주류 판매',\n",
       "  '식당-주차 가능',\n",
       "  '식당-지역',\n",
       "  '식당-흡연 가능',\n",
       "  '지하철-도착지',\n",
       "  '지하철-출발 시간',\n",
       "  '지하철-출발지',\n",
       "  '택시-도착 시간',\n",
       "  '택시-도착지',\n",
       "  '택시-종류',\n",
       "  '택시-출발 시간',\n",
       "  '택시-출발지'],\n",
       " 'is_last_turn': False,\n",
       " 'op2id': {'delete': 0, 'update': 1, 'dontcare': 2, 'carryover': 3},\n",
       " 'update_id': 1,\n",
       " 'slot_id': 35000,\n",
       " 'mask_id': 4,\n",
       " 'domain_id': 2,\n",
       " 'op_ids': [3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3],\n",
       " 'generate_ids': [[8784, 3]],\n",
       " 'input_id_g': [[17, 8784, 3]],\n",
       " 'input_mask_rel_pos': [[0, 1]],\n",
       " 'lm_label_ids': [[8784, 3]],\n",
       " 'input_id_g_max_len': 3,\n",
       " 'gen_max_len': 2,\n",
       " 'i_to_update': {24},\n",
       " 'i_dslen_map': {0: 4,\n",
       "  1: 3,\n",
       "  2: 3,\n",
       "  3: 3,\n",
       "  4: 3,\n",
       "  5: 2,\n",
       "  6: 2,\n",
       "  7: 3,\n",
       "  8: 2,\n",
       "  9: 2,\n",
       "  10: 3,\n",
       "  11: 3,\n",
       "  12: 3,\n",
       "  13: 3,\n",
       "  14: 4,\n",
       "  15: 3,\n",
       "  16: 2,\n",
       "  17: 3,\n",
       "  18: 3,\n",
       "  19: 2,\n",
       "  20: 3,\n",
       "  21: 2,\n",
       "  22: 4,\n",
       "  23: 3,\n",
       "  24: 2,\n",
       "  25: 3,\n",
       "  26: 4,\n",
       "  27: 4,\n",
       "  28: 3,\n",
       "  29: 3,\n",
       "  30: 2,\n",
       "  31: 3,\n",
       "  32: 2,\n",
       "  33: 3,\n",
       "  34: 3,\n",
       "  35: 2,\n",
       "  36: 3,\n",
       "  37: 3,\n",
       "  38: 3,\n",
       "  39: 3,\n",
       "  40: 3,\n",
       "  41: 3,\n",
       "  42: 2,\n",
       "  43: 3,\n",
       "  44: 3},\n",
       " 'n_updates': 1,\n",
       " 'diag_1_len': 26,\n",
       " 'diag_len': 72,\n",
       " 'input_id_p': [2,\n",
       "  31,\n",
       "  11655,\n",
       "  4279,\n",
       "  8553,\n",
       "  16,\n",
       "  3288,\n",
       "  4070,\n",
       "  6292,\n",
       "  6265,\n",
       "  10097,\n",
       "  6677,\n",
       "  4270,\n",
       "  4139,\n",
       "  4244,\n",
       "  11958,\n",
       "  4073,\n",
       "  3249,\n",
       "  4034,\n",
       "  8150,\n",
       "  4292,\n",
       "  3430,\n",
       "  4219,\n",
       "  5158,\n",
       "  7933,\n",
       "  3,\n",
       "  11655,\n",
       "  4279,\n",
       "  8553,\n",
       "  16,\n",
       "  6358,\n",
       "  7890,\n",
       "  4234,\n",
       "  8150,\n",
       "  4292,\n",
       "  3430,\n",
       "  4154,\n",
       "  4114,\n",
       "  4065,\n",
       "  4150,\n",
       "  35,\n",
       "  31,\n",
       "  7890,\n",
       "  4150,\n",
       "  35,\n",
       "  3234,\n",
       "  18,\n",
       "  18,\n",
       "  18,\n",
       "  7890,\n",
       "  4034,\n",
       "  14053,\n",
       "  18781,\n",
       "  6452,\n",
       "  4007,\n",
       "  6397,\n",
       "  4151,\n",
       "  4150,\n",
       "  18,\n",
       "  6677,\n",
       "  4270,\n",
       "  24387,\n",
       "  4275,\n",
       "  4155,\n",
       "  8784,\n",
       "  4283,\n",
       "  2084,\n",
       "  4007,\n",
       "  4480,\n",
       "  4311,\n",
       "  7489,\n",
       "  3,\n",
       "  35000,\n",
       "  6728,\n",
       "  21170,\n",
       "  3311,\n",
       "  4112,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  6728,\n",
       "  6295,\n",
       "  4199,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  6728,\n",
       "  17502,\n",
       "  6259,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  6728,\n",
       "  6336,\n",
       "  6969,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  6728,\n",
       "  6495,\n",
       "  4199,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  6728,\n",
       "  6479,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  6728,\n",
       "  7890,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  6728,\n",
       "  8117,\n",
       "  6259,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  6728,\n",
       "  6249,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  12175,\n",
       "  27672,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  12175,\n",
       "  17502,\n",
       "  6259,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  12175,\n",
       "  14561,\n",
       "  16184,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  12175,\n",
       "  9185,\n",
       "  16184,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  12175,\n",
       "  8866,\n",
       "  6551,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  12175,\n",
       "  8866,\n",
       "  2675,\n",
       "  4141,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  12175,\n",
       "  8866,\n",
       "  26090,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  12175,\n",
       "  6479,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  12175,\n",
       "  6811,\n",
       "  6259,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  12175,\n",
       "  24115,\n",
       "  6259,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  12175,\n",
       "  7890,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  12175,\n",
       "  8117,\n",
       "  6259,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  12175,\n",
       "  6249,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  12175,\n",
       "  12487,\n",
       "  4048,\n",
       "  16184,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  12175,\n",
       "  11707,\n",
       "  6259,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  8150,\n",
       "  27672,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  8150,\n",
       "  17502,\n",
       "  6259,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  8150,\n",
       "  11431,\n",
       "  4221,\n",
       "  16184,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  8150,\n",
       "  8866,\n",
       "  2675,\n",
       "  4141,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  8150,\n",
       "  8866,\n",
       "  6251,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  8150,\n",
       "  8866,\n",
       "  26090,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  8150,\n",
       "  6479,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  8150,\n",
       "  6811,\n",
       "  6259,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  8150,\n",
       "  7890,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  8150,\n",
       "  10472,\n",
       "  6477,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  8150,\n",
       "  8117,\n",
       "  6259,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  8150,\n",
       "  6249,\n",
       "  17,\n",
       "  6265,\n",
       "  10097,\n",
       "  35000,\n",
       "  8150,\n",
       "  11707,\n",
       "  6259,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  8779,\n",
       "  7520,\n",
       "  4200,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  8779,\n",
       "  7708,\n",
       "  6251,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  8779,\n",
       "  7708,\n",
       "  4200,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  8526,\n",
       "  7520,\n",
       "  6251,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  8526,\n",
       "  7520,\n",
       "  4200,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  8526,\n",
       "  7890,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  8526,\n",
       "  7708,\n",
       "  6251,\n",
       "  17,\n",
       "  35001,\n",
       "  35000,\n",
       "  8526,\n",
       "  7708,\n",
       "  4200,\n",
       "  17,\n",
       "  35001],\n",
       " 'input_id_p_len': 336,\n",
       " 'segment_id_p': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2],\n",
       " 'segment_id_g': [[3, 3, 3]],\n",
       " 'position_ids_g': [[336, 337, 338]]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_data.data[250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k_6Geon77ATQ",
    "outputId": "43bce168-7c67-483e-988f-39307b2b4986"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### word index of '-',  17\n"
     ]
    }
   ],
   "source": [
    "model_config = BertConfig.from_json_file(args.bert_config_path)\n",
    "model_config.dropout = args.dropout\n",
    "model_config.attention_probs_dropout_prob = args.attention_probs_dropout_prob\n",
    "model_config.hidden_dropout_prob = args.hidden_dropout_prob\n",
    "\n",
    "type_vocab_size = 4\n",
    "dec_config = args\n",
    "model = TransformerDST(model_config, dec_config, len(op2id), len(domain2id),\n",
    "                        op2id['update'],\n",
    "                        tokenizer.convert_tokens_to_ids(['[MASK]'])[0],\n",
    "                        tokenizer.convert_tokens_to_ids(['[SEP]'])[0],\n",
    "                        tokenizer.convert_tokens_to_ids(['[PAD]'])[0],\n",
    "                        tokenizer.convert_tokens_to_ids(['-'])[0],\n",
    "                        type_vocab_size, args.exclude_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BFzriwtR748D",
    "outputId": "264e8134-7242-47da-c4f3-7311c2674e8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.type_vocab_size != state_dict[bert.embeddings.token_type_embeddings.weight] (4 != 2)\n",
      "\n",
      "### Done Load BERT\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(args.bert_ckpt_path):\n",
    "    args.bert_ckpt_path = download_ckpt(args.bert_ckpt_path, args.bert_config_path, 'assets')\n",
    "\n",
    "state_dict = torch.load(args.bert_ckpt_path, map_location='cpu')\n",
    "_k = 'embeddings.token_type_embeddings.weight'\n",
    "print(\"config.type_vocab_size != state_dict[bert.embeddings.token_type_embeddings.weight] ({0} != {1})\".format(\n",
    "        type_vocab_size, state_dict[_k].shape[0]))\n",
    "# state_dict[_k].repeat(\n",
    "#     type_vocab_size, state_dict[_k].shape[1])\n",
    "state_dict[_k] = state_dict[_k].repeat(int(type_vocab_size/state_dict[_k].shape[0]), 1)\n",
    "state_dict[_k].data[2, :].copy_(state_dict[_k].data[0, :])\n",
    "state_dict[_k].data[3, :].copy_(state_dict[_k].data[0, :])\n",
    "model.bert.load_state_dict(state_dict)\n",
    "print(\"\\n### Done Load BERT\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n19E3DN3AHlH",
    "outputId": "a4f8e8bd-f109-453f-d683-8be4233e7e3d",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDST(\n",
       "  (bert): CustomBertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(35002, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(4, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (encoder): TransformerEncoder(\n",
       "    (bert_model): CustomBertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(35002, 768)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(4, 768)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (action_cls): Linear(in_features=768, out_features=4, bias=True)\n",
       "    (domain_cls): Linear(in_features=768, out_features=5, bias=True)\n",
       "  )\n",
       "  (decoder): BertForSeq2SeqDecoder(\n",
       "    (bert_model): CustomBertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(35002, 768)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(4, 768)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (predictions): CustomBertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=False)\n",
       "    )\n",
       "    (crit_mask_lm): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re-initialize added special tokens ([SLOT], [NULL], [EOS])\n",
    "model.bert.embeddings.word_embeddings.weight.data[1].normal_(mean=0.0, std=0.02)\n",
    "model.bert.embeddings.word_embeddings.weight.data[2].normal_(mean=0.0, std=0.02)\n",
    "model.bert.embeddings.word_embeddings.weight.data[3].normal_(mean=0.0, std=0.02)\n",
    "\n",
    "# re-initialize seg-2, seg-3\n",
    "model.bert.embeddings.token_type_embeddings.weight.data[2].normal_(mean=0.0, std=0.02)\n",
    "model.bert.embeddings.token_type_embeddings.weight.data[3].normal_(mean=0.0, std=0.02)\n",
    "model.bert.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mj5Y_hC1ASd3",
    "outputId": "e4ac6f71-3153-4404-e6b7-12ae7add54d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Use One Optim\n"
     ]
    }
   ],
   "source": [
    "num_train_steps = int(len(train_data_raw) / args.batch_size * args.n_epochs)\n",
    "\n",
    "if args.use_one_optim:\n",
    "    print(\"### Use One Optim\")\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(\n",
    "            nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(\n",
    "            nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.enc_lr)\n",
    "    scheduler = WarmupLinearSchedule(optimizer, int(num_train_steps * args.enc_warmup),\n",
    "                                          t_total=num_train_steps)\n",
    "else:\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    enc_param_optimizer = list(model.bert.named_parameters())  # TODO: For BERT only\n",
    "    print('### Optim BERT: {:}'.format(len(enc_param_optimizer)))\n",
    "    enc_optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in enc_param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in enc_param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "    enc_optimizer = AdamW(enc_optimizer_grouped_parameters, lr=args.enc_lr)\n",
    "    enc_scheduler = WarmupLinearSchedule(enc_optimizer, int(num_train_steps * args.enc_warmup),\n",
    "                                          t_total=num_train_steps)\n",
    "\n",
    "    dec_param_optimizer = list(model.named_parameters())  # TODO:  For other parameters\n",
    "    print('### Optim All: {:}'.format(len(dec_param_optimizer)))\n",
    "    dec_param_optimizer = [p for (n, p) in dec_param_optimizer if 'bert' not in n]\n",
    "    print('### Optim OTH: {:}'.format(len(dec_param_optimizer)))\n",
    "    dec_optimizer = AdamW(dec_param_optimizer, lr=args.dec_lr)\n",
    "    dec_scheduler = WarmupLinearSchedule(dec_optimizer, int(num_train_steps * args.dec_warmup),\n",
    "                                          t_total=num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "S9RJEfsNBN1q"
   },
   "outputs": [],
   "source": [
    "if n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data,\n",
    "                              sampler=train_sampler,\n",
    "                              batch_size=args.batch_size,\n",
    "                              collate_fn=train_data.collate_fn,\n",
    "                              num_workers=args.num_workers,\n",
    "                              worker_init_fn=worker_init_fn)\n",
    "\n",
    "loss_fnc = nn.CrossEntropyLoss()\n",
    "best_score = {'epoch': 0, 'joint_acc': 0, 'op_acc': 0, 'final_slot_f1': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tHnF8NT_5AEA",
    "outputId": "e0b88bea-cdd3-4687-9260-e1a975aa036b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 0.0 min, [1/30] [0/3587] mean_loss : 13.230, state_loss : 1.240, gen_loss : 10.489, dom_loss : 1.501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pytorch_transformers/optimization.py:160: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 1.3 min, [1/30] [100/3587] mean_loss : 13.196, state_loss : 0.881, gen_loss : 10.284, dom_loss : 1.651\n",
      "time 2.7 min, [1/30] [200/3587] mean_loss : 11.833, state_loss : 0.348, gen_loss : 9.155, dom_loss : 1.489\n",
      "time 4.0 min, [1/30] [300/3587] mean_loss : 10.122, state_loss : 0.300, gen_loss : 8.022, dom_loss : 1.442\n",
      "time 5.3 min, [1/30] [400/3587] mean_loss : 9.084, state_loss : 0.256, gen_loss : 6.968, dom_loss : 1.392\n",
      "time 6.7 min, [1/30] [500/3587] mean_loss : 8.664, state_loss : 0.187, gen_loss : 6.865, dom_loss : 1.405\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5d68ea7e3653>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(args.n_epochs):\n",
    "    batch_loss = []\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        batch = [b.to(device) if (not isinstance(b, int)) and (not isinstance(b, dict) and (not isinstance(b, list)) and (not isinstance(b, np.ndarray))) else b for b in batch]\n",
    "\n",
    "        input_ids_p, segment_ids_p, input_mask_p, \\\n",
    "        state_position_ids, op_ids, domain_ids, input_ids_g, segment_ids_g, position_ids_g, input_mask_g, \\\n",
    "        masked_pos, masked_weights, lm_label_ids, id_n_map, gen_max_len, n_total_pred = batch\n",
    "\n",
    "        domain_scores, state_scores, loss_g = model(input_ids_p, segment_ids_p, input_mask_p, state_position_ids,\n",
    "            input_ids_g, segment_ids_g, position_ids_g, input_mask_g,\n",
    "            masked_pos, masked_weights, lm_label_ids, id_n_map, gen_max_len, only_pred_op=args.only_pred_op, n_gpu=n_gpu)\n",
    "\n",
    "        if n_total_pred > 0:\n",
    "            loss_g = loss_g.sum() / n_total_pred\n",
    "        else:\n",
    "            loss_g = 0\n",
    "\n",
    "        loss_s = loss_fnc(state_scores.view(-1, len(op2id)), op_ids.view(-1))\n",
    "\n",
    "        if args.only_pred_op:\n",
    "            loss = loss_s\n",
    "        else:\n",
    "            loss = loss_s + loss_g\n",
    "\n",
    "        if args.exclude_domain is not True:\n",
    "            loss_d = loss_fnc(domain_scores.view(-1, len(domain2id)), domain_ids.view(-1))\n",
    "            loss = loss + loss_d\n",
    "\n",
    "        batch_loss.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        if args.use_one_optim:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        else:\n",
    "            enc_optimizer.step()\n",
    "            enc_scheduler.step()\n",
    "            dec_optimizer.step()\n",
    "            dec_scheduler.step()\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            try:\n",
    "                loss_g = loss_g.item()\n",
    "            except AttributeError:\n",
    "                loss_g = loss_g\n",
    "\n",
    "            if args.exclude_domain is not True:\n",
    "                print(\"time %.1f min, [%d/%d] [%d/%d] mean_loss : %.3f, state_loss : %.3f, gen_loss : %.3f, dom_loss : %.3f\" \\\n",
    "                      % ((time.time()-start_time)/60, epoch+1, args.n_epochs, step,\n",
    "                          len(train_dataloader), np.mean(batch_loss),\n",
    "                          loss_s.item(), loss_g, loss_d.item()))\n",
    "            else:\n",
    "                print(\"time %.1f min, [%d/%d] [%d/%d] mean_loss : %.3f, state_loss : %.3f, gen_loss : %.3f\" \\\n",
    "                      % ((time.time()-start_time)/60, epoch+1, args.n_epochs, step,\n",
    "                          len(train_dataloader), np.mean(batch_loss),\n",
    "                          loss_s.item(), loss_g))\n",
    "\n",
    "            sys.stdout.flush()\n",
    "            batch_loss = []\n",
    "\n",
    "    if args.use_one_optim:\n",
    "        save(args, epoch + 1, model, optimizer)\n",
    "    else:\n",
    "        save(args, epoch + 1, model, enc_optimizer, dec_optimizer)\n",
    "\n",
    "    if ((epoch+1) % args.eval_epoch == 0) and (epoch+1 >= 8):\n",
    "        eval_res = model_evaluation(model, dev_data_raw, tokenizer, slot_meta, epoch+1, args.op_code,\n",
    "                                    use_full_slot=args.use_full_slot, use_dt_only=args.use_dt_only, no_dial=args.no_dial, use_cls_only=args.use_cls_only, n_gpu=n_gpu)\n",
    "        print(\"### Epoch {:} Score : \".format(epoch+1), eval_res)\n",
    "\n",
    "        if eval_res['joint_acc'] > best_score['joint_acc']:\n",
    "            best_score = eval_res\n",
    "            print(\"### Best Joint Acc: {:} ###\".format(best_score['joint_acc']))\n",
    "            print('\\n')\n",
    "\n",
    "            if epoch+1 >= 8:  # To speed up\n",
    "                eval_res_test = model_evaluation(model, test_data_raw, tokenizer, slot_meta, epoch + 1, args.op_code,\n",
    "                                                  use_full_slot=args.use_full_slot, use_dt_only=args.use_dt_only, no_dial=args.no_dial, use_cls_only=args.use_cls_only, n_gpu=n_gpu)\n",
    "                print(\"### Epoch {:} Test Score : \".format(epoch + 1), eval_res_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-03-22T03:53:49.161680Z",
     "iopub.status.busy": "2021-03-22T03:53:49.160993Z",
     "iopub.status.idle": "2021-03-22T03:53:50.162602Z",
     "shell.execute_reply": "2021-03-22T03:53:50.161776Z",
     "shell.execute_reply.started": "2021-03-22T03:53:49.161607Z"
    },
    "id": "O-6Xn5G504XD",
    "outputId": "44450e55-dade-4564-d9ac-b218fb91146c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6301/6301 [00:00<00:00, 11783.13it/s]\n",
      "100%|██████████| 699/699 [00:00<00:00, 16536.95it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data_file = \"/content/drive/MyDrive/WOS/train_dials.json\"\n",
    "slot_meta = json.load(open(\"/content/drive/MyDrive/WOS/slot_meta.json\"))\n",
    "ontology = json.load(open(\"/content/drive/MyDrive/WOS/ontology.json\"))\n",
    "train_data, dev_data, dev_labels = load_dataset(train_data_file)\n",
    "\n",
    "train_examples = get_examples_from_dialogues(train_data,\n",
    "                                             user_first=False,\n",
    "                                             dialogue_level=False)\n",
    "dev_examples = get_examples_from_dialogues(dev_data,\n",
    "                                           user_first=False,\n",
    "                                           dialogue_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6C9sqEXz04XE",
    "outputId": "d2a16a25-e5b1-4d34-8e9d-498f5cc555e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46294\n",
      "4951\n"
     ]
    }
   ],
   "source": [
    "print(len(train_examples))\n",
    "print(len(dev_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t5pAWkQl_JRG",
    "outputId": "7aac628c-e5cd-42d2-db4a-7a39dc077ad6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DSTInputExample(guid='snowy-hat-8324:관광_식당_11-0', context_turns=[], current_turn=['', '서울 중앙에 있는 박물관을 찾아주세요'], label=['관광-종류-박물관', '관광-지역-서울 중앙'])"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hk2bVKMH04XE"
   },
   "source": [
    "## TRADE Preprocessor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SaR5NuDp04XE"
   },
   "source": [
    "기존의 GRU 기반의 인코더를 BERT-based Encoder로 바꿀 준비를 합시다.\n",
    "\n",
    "1. 현재 `_convert_example_to_feature`에서는 `max_seq_length`를 핸들하고 있지 않습니다. `input_id`와 `segment_id`가 `max_seq_length`를 넘어가면 좌측부터 truncate시키는 코드를 삽입하세요.\n",
    "\n",
    "2. hybrid approach에서 얻은 교훈을 바탕으로 gate class를 3개에서 5개로 늘려봅시다.\n",
    "    - `gating2id`를 수정하세요\n",
    "    - 이에 따른 `recover_state`를 수정하세요.\n",
    "    \n",
    "3. word dropout을 구현하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T03:53:51.145056Z",
     "iopub.status.busy": "2021-03-22T03:53:51.144807Z",
     "iopub.status.idle": "2021-03-22T03:53:51.161809Z",
     "shell.execute_reply": "2021-03-22T03:53:51.161180Z",
     "shell.execute_reply.started": "2021-03-22T03:53:51.145035Z"
    },
    "id": "Bo89P9d304XF"
   },
   "outputs": [],
   "source": [
    "class TRADEPreprocessor(DSTPreprocessor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        slot_meta,\n",
    "        src_tokenizer,\n",
    "        trg_tokenizer=None,\n",
    "        ontology=None,\n",
    "        max_seq_length=512,\n",
    "    ):\n",
    "        self.slot_meta = slot_meta\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.trg_tokenizer = trg_tokenizer if trg_tokenizer else src_tokenizer\n",
    "        self.ontology = ontology\n",
    "        self.gating2id = {\"none\": 0, \"dontcare\": 1, \"yes\": 2, \"no\": 3, \"ptr\": 4}\n",
    "        self.id2gating = {v: k for k, v in self.gating2id.items()}\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def _convert_example_to_feature(self, example):\n",
    "        dialogue_context = \" [SEP] \".join(example.context_turns + example.current_turn)\n",
    "\n",
    "        input_id = self.src_tokenizer.encode(dialogue_context, add_special_tokens=False)\n",
    "        max_length = self.max_seq_length - 2\n",
    "        if len(input_id) > max_length:\n",
    "            gap = len(input_id) - max_length\n",
    "            input_id = input_id[gap:]\n",
    "\n",
    "        input_id = (\n",
    "            [self.src_tokenizer.cls_token_id]\n",
    "            + input_id\n",
    "            + [self.src_tokenizer.sep_token_id]\n",
    "        )\n",
    "        segment_id = [0] * len(input_id)\n",
    "\n",
    "        target_ids = []\n",
    "        gating_id = []\n",
    "        if not example.label:\n",
    "            example.label = []\n",
    "\n",
    "        state = convert_state_dict(example.label)\n",
    "        for slot in self.slot_meta:\n",
    "            value = state.get(slot, \"none\")\n",
    "            target_id = self.trg_tokenizer.encode(value, add_special_tokens=False) + [\n",
    "                self.trg_tokenizer.sep_token_id\n",
    "            ]\n",
    "            target_ids.append(target_id)\n",
    "            gating_id.append(self.gating2id.get(value, self.gating2id[\"ptr\"]))\n",
    "        target_ids = self.pad_ids(target_ids, self.trg_tokenizer.pad_token_id)\n",
    "        return OpenVocabDSTFeature(\n",
    "            example.guid, input_id, segment_id, gating_id, target_ids\n",
    "        )\n",
    "\n",
    "    def convert_examples_to_features(self, examples):\n",
    "        return list(map(self._convert_example_to_feature, tqdm(examples)))\n",
    "\n",
    "    def recover_state(self, gate_list, gen_list):\n",
    "        assert len(gate_list) == len(self.slot_meta)\n",
    "        assert len(gen_list) == len(self.slot_meta)\n",
    "\n",
    "        recovered = []\n",
    "        for slot, gate, value in zip(self.slot_meta, gate_list, gen_list):\n",
    "            if self.id2gating[gate] == \"none\":\n",
    "                continue\n",
    "\n",
    "            if self.id2gating[gate] in [\"dontcare\", \"yes\", \"no\"]:\n",
    "                recovered.append(\"%s-%s\" % (slot, self.id2gating[gate]))\n",
    "                continue\n",
    "\n",
    "            token_id_list = []\n",
    "            for id_ in value:\n",
    "                if id_ in self.trg_tokenizer.all_special_ids:\n",
    "                    break\n",
    "\n",
    "                token_id_list.append(id_)\n",
    "            value = self.trg_tokenizer.decode(token_id_list, skip_special_tokens=True)\n",
    "\n",
    "            if value == \"none\":\n",
    "                continue\n",
    "\n",
    "            recovered.append(\"%s-%s\" % (slot, value))\n",
    "        return recovered\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        guids = [b.guid for b in batch]\n",
    "        input_ids = torch.LongTensor(\n",
    "            self.pad_ids([b.input_id for b in batch], self.src_tokenizer.pad_token_id)\n",
    "        )\n",
    "        segment_ids = torch.LongTensor(\n",
    "            self.pad_ids([b.segment_id for b in batch], self.src_tokenizer.pad_token_id)\n",
    "        )\n",
    "        input_masks = input_ids.ne(self.src_tokenizer.pad_token_id)\n",
    "\n",
    "        gating_ids = torch.LongTensor([b.gating_id for b in batch])\n",
    "        target_ids = self.pad_id_of_matrix(\n",
    "            [torch.LongTensor(b.target_ids) for b in batch],\n",
    "            self.trg_tokenizer.pad_token_id,\n",
    "        )\n",
    "        return input_ids, segment_ids, input_masks, gating_ids, target_ids, guids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wd47XExT04XF"
   },
   "source": [
    "## Convert_Examples_to_Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-03-22T03:53:57.277499Z",
     "iopub.status.busy": "2021-03-22T03:53:57.276828Z",
     "iopub.status.idle": "2021-03-22T03:58:02.047206Z",
     "shell.execute_reply": "2021-03-22T03:58:02.046258Z",
     "shell.execute_reply.started": "2021-03-22T03:53:57.277443Z"
    },
    "id": "zOjho3J404XG",
    "outputId": "39005dfe-8139-490d-8c18-dd664d6bd4d0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 54/46294 [00:00<04:22, 176.14it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 46294/46294 [04:37<00:00, 166.83it/s]\n",
      "100%|██████████| 4951/4951 [00:28<00:00, 172.46it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('dsksd/bert-ko-small-minimal')\n",
    "processor = TRADEPreprocessor(slot_meta, tokenizer, max_seq_length=512)\n",
    "\n",
    "train_features = processor.convert_examples_to_features(train_examples)\n",
    "dev_features = processor.convert_examples_to_features(dev_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFNdbCsB04XG",
    "outputId": "fb7b0fa6-3d1c-4a19-9a33-53e46e0a88b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46255\n",
      "4990\n"
     ]
    }
   ],
   "source": [
    "print(len(train_features))\n",
    "print(len(dev_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P8nDSMMc_EAQ",
    "outputId": "f4bddc4e-496c-4fb9-f01f-c6179e7e1d40"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenVocabDSTFeature(guid='polished-poetry-0057:관광_9-2', input_id=[2, 3, 7596, 4292, 3755, 4228, 18781, 6265, 10806, 4073, 3249, 11649, 4150, 35, 3, 6265, 10806, 4073, 7596, 4007, 6259, 4283, 2084, 4007, 24874, 28060, 16301, 15550, 12178, 4007, 3249, 4576, 6216, 18, 3, 3158, 2279, 7149, 9068, 3305, 6449, 4076, 8553, 18, 3, 28060, 16301, 15550, 12178, 4234, 9068, 4034, 6265, 27439, 10732, 11684, 4096, 10561, 18, 3, 6449, 4076, 4114, 4034, 4396, 4073, 20025, 4294, 18790, 4086, 3305, 6449, 4076, 8553, 18, 3], segment_id=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], gating_id=[0, 0, 0, 0, 0, 4, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], target_ids=[[21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [28060, 16301, 15550, 12178, 3], [7596, 3, 0, 0, 0], [21832, 11764, 3, 0, 0], [6265, 10806, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0], [21832, 11764, 3, 0, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0tft4I6F_nl4",
    "outputId": "54764df6-24ae-45d5-b6e0-2d2f9f284831"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', '노량진 수산물 도매시장 [SEP]', '쇼핑 [SEP] [PAD] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', '서울 서쪽 [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]', 'none [SEP] [PAD] [PAD]']\n"
     ]
    }
   ],
   "source": [
    "lst = [tokenizer.decode(x) for x in train_features[10].target_ids]\n",
    "print(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovQOZ9Sz04XH"
   },
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaivCGee04XH"
   },
   "source": [
    "1. `GRUEncoder`를 `BertModel`로 교체하세요. 이에 따라 `tie_weight` 함수가 수정되어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T03:58:02.049196Z",
     "iopub.status.busy": "2021-03-22T03:58:02.048851Z",
     "iopub.status.idle": "2021-03-22T03:58:02.074778Z",
     "shell.execute_reply": "2021-03-22T03:58:02.074001Z",
     "shell.execute_reply.started": "2021-03-22T03:58:02.049167Z"
    },
    "id": "A7M4oZ1w04XH"
   },
   "outputs": [],
   "source": [
    "class TRADE(nn.Module):\n",
    "    def __init__(self, config, slot_vocab, slot_meta, pad_idx=0):\n",
    "        super(TRADE, self).__init__()\n",
    "        self.slot_meta = slot_meta\n",
    "        if config.model_name_or_path:\n",
    "            self.encoder = BertModel.from_pretrained(config.model_name_or_path)\n",
    "        else:\n",
    "            self.encoder = BertModel(config)\n",
    "            \n",
    "        self.decoder = SlotGenerator(\n",
    "            config.vocab_size,\n",
    "            config.hidden_size,\n",
    "            config.hidden_dropout_prob,\n",
    "            config.n_gate,\n",
    "            None,\n",
    "            pad_idx,\n",
    "        )\n",
    "        \n",
    "        # init for only subword embedding\n",
    "        self.decoder.set_slot_idx(slot_vocab)\n",
    "        self.tie_weight()\n",
    "\n",
    "    def tie_weight(self):\n",
    "        self.decoder.embed.weight = self.encoder.embeddings.word_embeddings.weight\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask=None, max_len=10, teacher=None):\n",
    "\n",
    "        encoder_outputs, pooled_output = self.encoder(input_ids=input_ids)\n",
    "        all_point_outputs, all_gate_outputs = self.decoder(\n",
    "            input_ids, encoder_outputs, pooled_output.unsqueeze(0), attention_mask, max_len, teacher\n",
    "        )\n",
    "\n",
    "        return all_point_outputs, all_gate_outputs\n",
    "    \n",
    "class SlotGenerator(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size, hidden_size, dropout, n_gate, proj_dim=None, pad_idx=0\n",
    "    ):\n",
    "        super(SlotGenerator, self).__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed = nn.Embedding(\n",
    "            vocab_size, hidden_size, padding_idx=pad_idx\n",
    "        )  # shared with encoder\n",
    "\n",
    "        if proj_dim:\n",
    "            self.proj_layer = nn.Linear(hidden_size, proj_dim, bias=False)\n",
    "        else:\n",
    "            self.proj_layer = None\n",
    "        self.hidden_size = proj_dim if proj_dim else hidden_size\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            self.hidden_size, self.hidden_size, 1, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.n_gate = n_gate\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.w_gen = nn.Linear(self.hidden_size * 3, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.w_gate = nn.Linear(self.hidden_size, n_gate)\n",
    "\n",
    "    def set_slot_idx(self, slot_vocab_idx):\n",
    "        whole = []\n",
    "        max_length = max(map(len, slot_vocab_idx))\n",
    "        for idx in slot_vocab_idx:\n",
    "            if len(idx) < max_length:\n",
    "                gap = max_length - len(idx)\n",
    "                idx.extend([self.pad_idx] * gap)\n",
    "            whole.append(idx)\n",
    "        self.slot_embed_idx = whole  # torch.LongTensor(whole)\n",
    "\n",
    "    def embedding(self, x):\n",
    "        x = self.embed(x)\n",
    "        if self.proj_layer:\n",
    "            x = self.proj_layer(x)\n",
    "        return x\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids, encoder_output, hidden, input_masks, max_len, teacher=None\n",
    "    ):\n",
    "        input_masks = input_masks.ne(1)\n",
    "        # J, slot_meta : key : [domain, slot] ex> LongTensor([1,2])\n",
    "        # J,2\n",
    "        batch_size = encoder_output.size(0)\n",
    "        slot = torch.LongTensor(self.slot_embed_idx).to(input_ids.device)  ##\n",
    "        slot_e = torch.sum(self.embedding(slot), 1)  # J,d\n",
    "        J = slot_e.size(0)\n",
    "\n",
    "        all_point_outputs = torch.zeros(batch_size, J, max_len, self.vocab_size).to(\n",
    "            input_ids.device\n",
    "        )\n",
    "        \n",
    "        # Parallel Decoding\n",
    "        w = slot_e.repeat(batch_size, 1).unsqueeze(1)\n",
    "        hidden = hidden.repeat_interleave(J, dim=1)\n",
    "        encoder_output = encoder_output.repeat_interleave(J, dim=0)\n",
    "        input_ids = input_ids.repeat_interleave(J, dim=0)\n",
    "        input_masks = input_masks.repeat_interleave(J, dim=0)\n",
    "        for k in range(max_len):\n",
    "            w = self.dropout(w)\n",
    "            _, hidden = self.gru(w, hidden)  # 1,B,D\n",
    "\n",
    "            # B,T,D * B,D,1 => B,T\n",
    "            attn_e = torch.bmm(encoder_output, hidden.permute(1, 2, 0))  # B,T,1\n",
    "            attn_e = attn_e.squeeze(-1).masked_fill(input_masks, -1e9)\n",
    "            attn_history = F.softmax(attn_e, -1)  # B,T\n",
    "\n",
    "            if self.proj_layer:\n",
    "                hidden_proj = torch.matmul(hidden, self.proj_layer.weight)\n",
    "            else:\n",
    "                hidden_proj = hidden\n",
    "\n",
    "            # B,D * D,V => B,V\n",
    "            attn_v = torch.matmul(\n",
    "                hidden_proj.squeeze(0), self.embed.weight.transpose(0, 1)\n",
    "            )  # B,V\n",
    "            attn_vocab = F.softmax(attn_v, -1)\n",
    "\n",
    "            # B,1,T * B,T,D => B,1,D\n",
    "            context = torch.bmm(attn_history.unsqueeze(1), encoder_output)  # B,1,D\n",
    "            p_gen = self.sigmoid(\n",
    "                self.w_gen(torch.cat([w, hidden.transpose(0, 1), context], -1))\n",
    "            )  # B,1\n",
    "            p_gen = p_gen.squeeze(-1)\n",
    "\n",
    "            p_context_ptr = torch.zeros_like(attn_vocab).to(input_ids.device)\n",
    "            p_context_ptr.scatter_add_(1, input_ids, attn_history)  # copy B,V\n",
    "            p_final = p_gen * attn_vocab + (1 - p_gen) * p_context_ptr  # B,V\n",
    "            _, w_idx = p_final.max(-1)\n",
    "\n",
    "            if teacher is not None:\n",
    "                w = self.embedding(teacher[:, :, k]).transpose(0, 1).reshape(batch_size * J, 1, -1)\n",
    "            else:\n",
    "                w = self.embedding(w_idx).unsqueeze(1)  # B,1,D\n",
    "            if k == 0:\n",
    "                gated_logit = self.w_gate(context.squeeze(1))  # B,3\n",
    "                all_gate_outputs = gated_logit.view(batch_size, J, self.n_gate)\n",
    "            all_point_outputs[:, :, k, :] = p_final.view(batch_size, J, self.vocab_size)\n",
    "\n",
    "        return all_point_outputs, all_gate_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdLS3jkA04XI"
   },
   "source": [
    "# 모델 및 데이터 로더 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-MYTPvu004XI",
    "outputId": "4631f60a-11b1-450e-a5a8-8abbcaa3f225"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kyumin/Development/bc_dst/.venv/lib/python3.8/site-packages/torch/nn/modules/rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "slot_vocab = []\n",
    "for slot in slot_meta:\n",
    "    slot_vocab.append(\n",
    "        tokenizer.encode(slot.replace('-', ' '),\n",
    "                         add_special_tokens=False)\n",
    "    )\n",
    "    \n",
    "config = BertConfig.from_pretrained('dsksd/bert-ko-small-minimal')\n",
    "config.model_name_or_path = 'dsksd/bert-ko-small-minimal'\n",
    "config.n_gate = len(processor.gating2id)\n",
    "config.proj_dim = None\n",
    "model = TRADE(config, slot_vocab, slot_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T03:58:02.076414Z",
     "iopub.status.busy": "2021-03-22T03:58:02.076171Z",
     "iopub.status.idle": "2021-03-22T03:58:07.655001Z",
     "shell.execute_reply": "2021-03-22T03:58:07.654125Z",
     "shell.execute_reply.started": "2021-03-22T03:58:02.076394Z"
    },
    "id": "ksvFxR-h04XI"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_data = WOSDataset(train_features)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_loader = DataLoader(train_data, batch_size=16, sampler=train_sampler, collate_fn=processor.collate_fn)\n",
    "\n",
    "dev_data = WOSDataset(dev_features)\n",
    "dev_sampler = SequentialSampler(dev_data)\n",
    "dev_loader = DataLoader(dev_data, batch_size=8, sampler=dev_sampler, collate_fn=processor.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVYxBaXE04XJ"
   },
   "source": [
    "# Optimizer & Scheduler 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T03:58:07.656475Z",
     "iopub.status.busy": "2021-03-22T03:58:07.656216Z",
     "iopub.status.idle": "2021-03-22T03:58:12.190910Z",
     "shell.execute_reply": "2021-03-22T03:58:12.190106Z",
     "shell.execute_reply.started": "2021-03-22T03:58:07.656453Z"
    },
    "id": "NmDmrigU04XJ"
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.01,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "t_total = len(train_loader) * n_epochs\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=0.1, num_training_steps=t_total\n",
    ")\n",
    "teacher_forcing = 0.5\n",
    "model.to(device)\n",
    "\n",
    "def masked_cross_entropy_for_value(logits, target, pad_idx=0):\n",
    "    mask = target.ne(pad_idx)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    log_probs_flat = torch.log(logits_flat)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    losses = losses * mask.float()\n",
    "    loss = losses.sum() / (mask.sum().float())\n",
    "    return loss\n",
    "\n",
    "loss_fnc_1 = masked_cross_entropy_for_value  # generation\n",
    "loss_fnc_2 = nn.CrossEntropyLoss()  # gating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0GK3Z8z04XJ"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T03:58:12.192122Z",
     "iopub.status.busy": "2021-03-22T03:58:12.191880Z"
    },
    "id": "mrWnCWwC04XJ",
    "outputId": "194c265e-ab26-4993-a61e-1dca5d3274aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10] [0/2891] 10.265509\n",
      "[0/10] [100/2891] 1.779430\n",
      "[0/10] [200/2891] 1.638214\n",
      "[0/10] [300/2891] 1.447321\n",
      "[0/10] [400/2891] 1.126810\n",
      "[0/10] [500/2891] 1.047267\n",
      "[0/10] [600/2891] 1.076839\n",
      "[0/10] [700/2891] 0.928714\n",
      "[0/10] [800/2891] 0.688376\n",
      "[0/10] [900/2891] 0.605567\n",
      "[0/10] [1000/2891] 0.638915\n",
      "[0/10] [1100/2891] 0.530238\n",
      "[0/10] [1200/2891] 0.453068\n",
      "[0/10] [1300/2891] 0.450586\n",
      "[0/10] [1400/2891] 0.394243\n",
      "[0/10] [1500/2891] 0.290189\n",
      "[0/10] [1600/2891] 0.471834\n",
      "[0/10] [1700/2891] 0.314694\n",
      "[0/10] [1800/2891] 0.270238\n",
      "[0/10] [1900/2891] 0.414060\n",
      "[0/10] [2000/2891] 0.290958\n",
      "[0/10] [2100/2891] 0.350836\n",
      "[0/10] [2200/2891] 0.272087\n",
      "[0/10] [2300/2891] 0.359535\n",
      "[0/10] [2400/2891] 0.315303\n",
      "[0/10] [2500/2891] 0.296642\n",
      "[0/10] [2600/2891] 0.198743\n",
      "[0/10] [2700/2891] 0.247285\n",
      "[0/10] [2800/2891] 0.248392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 624/624 [01:09<00:00,  9.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joint_goal_accuracy': 0.30280561122244487, 'turn_slot_accuracy': 0.9667022934758523, 'turn_slot_f1': 0.8535316828673348}\n",
      "joint_goal_accuracy: 0.30280561122244487\n",
      "turn_slot_accuracy: 0.9667022934758523\n",
      "turn_slot_f1: 0.8535316828673348\n",
      "[1/10] [0/2891] 0.168331\n",
      "[1/10] [100/2891] 0.183329\n",
      "[1/10] [200/2891] 0.206472\n",
      "[1/10] [300/2891] 0.219990\n",
      "[1/10] [400/2891] 0.209132\n",
      "[1/10] [500/2891] 0.169938\n",
      "[1/10] [600/2891] 0.149240\n",
      "[1/10] [700/2891] 0.139512\n",
      "[1/10] [800/2891] 0.182836\n",
      "[1/10] [900/2891] 0.213563\n",
      "[1/10] [1000/2891] 0.205337\n",
      "[1/10] [1100/2891] 0.172299\n",
      "[1/10] [1200/2891] 0.110077\n",
      "[1/10] [1300/2891] 0.115137\n",
      "[1/10] [1400/2891] 0.226755\n",
      "[1/10] [1500/2891] 0.158137\n",
      "[1/10] [1600/2891] 0.102026\n",
      "[1/10] [1700/2891] 0.170833\n",
      "[1/10] [1800/2891] 0.142623\n",
      "[1/10] [1900/2891] 0.116227\n",
      "[1/10] [2000/2891] 0.121230\n",
      "[1/10] [2100/2891] 0.161562\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ab9c3fe622a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m        \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m        \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m        \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m        \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/bc_dst/.venv/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Development/bc_dst/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " for epoch in range(n_epochs):\n",
    "    batch_loss = []\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input_ids, segment_ids, input_masks, gating_ids, target_ids, guids = [b.to(device) if not isinstance(b, list) else b for b in batch]\n",
    "        if teacher_forcing > 0.0 and random.random() < teacher_forcing:\n",
    "            tf = target_ids\n",
    "        else:\n",
    "            tf = None\n",
    "\n",
    "        all_point_outputs, all_gate_outputs = model(input_ids, segment_ids, input_masks, target_ids.size(-1))  # gt - length (generation)\n",
    "        loss_1 = loss_fnc_1(all_point_outputs.contiguous(), target_ids.contiguous().view(-1))\n",
    "        loss_2 = loss_fnc_2(all_gate_outputs.contiguous().view(-1, 5), gating_ids.contiguous().view(-1))\n",
    "        loss = loss_1 + loss_2\n",
    "        batch_loss.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        if step % 100 == 0:\n",
    "            print('[%d/%d] [%d/%d] %f' % (epoch, n_epochs, step, len(train_loader), loss.item()))\n",
    "                \n",
    "    predictions = inference(model, dev_loader, processor, device)\n",
    "    eval_result = _evaluation(predictions, dev_labels, slot_meta)\n",
    "    for k, v in eval_result.items():\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UR4EEzbz04XK"
   },
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jOJqYcpP04XK",
    "outputId": "8947b1d7-fcdc-4c4b-8536-a8ee7a057123"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1936.46it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_data = json.load(open(f\"/opt/ml/input/data/eval_dataset/eval_dials.json\", \"r\"))\n",
    "\n",
    "eval_examples = get_examples_from_dialogues(\n",
    "    eval_data, user_first=False, dialogue_level=False\n",
    ")\n",
    "\n",
    "# Extracting Featrues\n",
    "eval_features = processor.convert_examples_to_features(eval_examples)\n",
    "eval_data = WOSDataset(eval_features)\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_loader = DataLoader(\n",
    "    eval_data,\n",
    "    batch_size=8,\n",
    "    sampler=eval_sampler,\n",
    "    collate_fn=processor.collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nLU16_Ur04XL",
    "outputId": "c30e4ae5-5212-41f8-f171-0ff5e56b02fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 944/944 [01:45<00:00,  8.92it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = inference(model, eval_loader, processor, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0uR9J8e04XL"
   },
   "outputs": [],
   "source": [
    "json.dump(predictions, open('predictions.csv', 'w'), indent=2, ensure_ascii=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhrO4LRQ04XL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stage3-09) Transformer-DST Preprocessor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "002f715aa4bd444ca40d15d40975b9b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1160ad01ed8b4abca76708edfd6f9fff",
       "IPY_MODEL_6405f7f3573049aebb815bccc057e089"
      ],
      "layout": "IPY_MODEL_4df3657b19324b46926ee324d77b890c"
     }
    },
    "0d063cc9981748359845a690ce4132ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1160ad01ed8b4abca76708edfd6f9fff": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e4ed712135542a1b75b276fe9dae691",
      "max": 263327,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_db57f006af5446ba8c402d2ecc8b3798",
      "value": 263327
     }
    },
    "1d91402b3e81484a8e806133ae0b2aa6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e4ed712135542a1b75b276fe9dae691": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "339aa73d924c4f34b20cf927e3acbad1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34ea407dcc794348aa33fc46a19a963a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7e8c27125cee4105a82fe0ba09e5d77e",
       "IPY_MODEL_e4e299a6984540e686940e7f763606c2"
      ],
      "layout": "IPY_MODEL_339aa73d924c4f34b20cf927e3acbad1"
     }
    },
    "4df3657b19324b46926ee324d77b890c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "50ea6b36f88d4c89bc58e9da3e03b274": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6405f7f3573049aebb815bccc057e089": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_50ea6b36f88d4c89bc58e9da3e03b274",
      "placeholder": "​",
      "style": "IPY_MODEL_d57b60747cd943568a22cb38eda92fae",
      "value": " 263k/263k [00:01&lt;00:00, 159kB/s]"
     }
    },
    "6816d95cb3b840f1bcd8bd169919bf60": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8fe163c517734c23b6fb15f72952c843",
      "max": 124,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b2f70e8ede5f4e63b16c83646d24e196",
      "value": 124
     }
    },
    "6e6e2940e6534671947b95268ccf0c38": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e8c27125cee4105a82fe0ba09e5d77e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a2d36a80b8f144cc815c7c00c6eab9f5",
      "max": 288,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_82812bfd15054564876b1cfdcd666c93",
      "value": 288
     }
    },
    "82812bfd15054564876b1cfdcd666c93": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "836b292e938e42c0a0c2caffd46e786b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6e6e2940e6534671947b95268ccf0c38",
      "placeholder": "​",
      "style": "IPY_MODEL_0d063cc9981748359845a690ce4132ce",
      "value": " 124/124 [00:00&lt;00:00, 254B/s]"
     }
    },
    "8fe163c517734c23b6fb15f72952c843": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a5445823b104e979787a61c19e4ee84": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2d36a80b8f144cc815c7c00c6eab9f5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2f70e8ede5f4e63b16c83646d24e196": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "be8cff8394b14d928d44c973286e9e19": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6816d95cb3b840f1bcd8bd169919bf60",
       "IPY_MODEL_836b292e938e42c0a0c2caffd46e786b"
      ],
      "layout": "IPY_MODEL_9a5445823b104e979787a61c19e4ee84"
     }
    },
    "d57b60747cd943568a22cb38eda92fae": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "db57f006af5446ba8c402d2ecc8b3798": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "dbb46730360748c98b336e5f24892276": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e4e299a6984540e686940e7f763606c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d91402b3e81484a8e806133ae0b2aa6",
      "placeholder": "​",
      "style": "IPY_MODEL_dbb46730360748c98b336e5f24892276",
      "value": " 288/288 [00:00&lt;00:00, 976B/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
